{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cd37fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#==================================================================\n",
    "#Program: DEVEL_3\n",
    "#Version: 1.0\n",
    "#Author: David Helminiak\n",
    "#Date Created: September 5, 2024\n",
    "#Date Last Modified: September 22, 2024\n",
    "#Description: Re-implementation of a unified block classifier code for breast cancer data\n",
    "#Operation: Move back into main program directory before running.\n",
    "#Status: Deprecated - Completed and transitioned into .py file structure\n",
    "#==================================================================\n",
    "\n",
    "#Have the notebook fill more of the display width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:80% !important; }</style>\"))\n",
    "\n",
    "#Items otherwise covered when not running this code in a notebook\n",
    "import tempfile\n",
    "dir_tmp = tempfile.TemporaryDirectory(prefix='TMP_').name\n",
    "configFileName = './CONFIG_0-TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3054df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#CONFIG PARAMETERS\n",
    "#==================================================================\n",
    "\n",
    "#TASKS\n",
    "#==================================================================\n",
    "#Should classification/cross-validation be performed on blocks and the portions of WSI they were extracted from \n",
    "classifierBlocks = True\n",
    "\n",
    "#Should classifier model components be saved/exported\n",
    "classifierExport = True\n",
    "\n",
    "#Should whole WSI be classified and evaluated\n",
    "classifierWSI = True\n",
    "\n",
    "#Should training data be generated for reconstruction model\n",
    "classifierRecon = True\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "#COMPUTE\n",
    "#==================================================================\n",
    "#How many samples should be submitted in a batch through pytorch models used in classifier\n",
    "#Incrementing in powers of 2 recommended to best leverage common GPU hardware designs\n",
    "#For ResNet and DenseNet a 2080TI 11GB can handle 64x3x224x224 (resizeSize=224) or 16x3x400x400 (resizeSize=0)\n",
    "#In some cases, the GPU memory may not be fully released (even when explicitly told to do so), so using a lower batch size may help prevent OOM\n",
    "batchsizeClassifier = 32\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "#CLASSIFICATION - BLOCKS\n",
    "#==================================================================\n",
    "#Should features be extracted for blocks and overwrite previously generated files\n",
    "overwrite_blocks_features = True\n",
    "\n",
    "#Should saliency maps be determined for blocks and overwrite previously generated files\n",
    "overwrite_blocks_saliencyMaps = True\n",
    "\n",
    "#Should the decision fusion mode be used for block classification (default: True)\n",
    "fusionMode_blocks = True\n",
    "\n",
    "#Should saliency maps and their overlays be visualized for block WSI\n",
    "visualizeSaliencyMaps_blocks = True\n",
    "\n",
    "#Should label grids and their overlays be visualized for block WSI; will overwrite previously generated files\n",
    "#Files should be updated if 1) thresholdWSI_GT is changed or 2) thresholdWSI is changed and thresholdWSI_GT is True\n",
    "visualizeLabelGrids_blocks = True\n",
    "\n",
    "#Should prediction grids and their overlays be visualized for block WSI\n",
    "visualizePredictionGrids_blocks = True\n",
    "\n",
    "#What ratio of malignant to benign blocks should be used to label a whole WSI as malignant (default: 0.15)\n",
    "#Unknown what the original work used for this value, but chose a value (0.15 - being in range of 0.12-0.19) that can replicate results from original work\n",
    "#If this value is changed and thresholdWSI_GT is True, then should enable visualizeLabelGrids_blocks to update stored data\n",
    "thresholdWSI = 0.15\n",
    "\n",
    "#Should the thresholdWSI be used to determine the ground-truth label (True) or use recorded metadata (False) (default: False)\n",
    "#Unsure exactly what the original work did, but suspect it may have used method equivalent of 'True', given its usage replicates the original results.\n",
    "#Strongly recommend using recorded metadata! The option has been left to enable replication of original results.\n",
    "#If this value is changed, then should enable visualizeLabelGrids_blocks to update stored data\n",
    "thresholdWSI_GT = False\n",
    "\n",
    "#If folds for XGB classifier cross validation should be manually defined (e.g. [['S1', 'S3'], ['S4', 'S2']]), else use specify number of folds to generate\n",
    "#Default matches folds used in prior work (https://doi.org/10.3389/fonc.2023.1179025)\n",
    "#Omits 6 available samples, with folds holding: 11, 12, 12, 12, 13 samples respectively; this may have been to better balance class distribution\n",
    "#Presently, all available (non-excluded) samples (not just those in manualFolds) are currently used for training the exported/utilized final classifier\n",
    "manualFolds = [['2_1', '9_3', '11_3', '16_3', '34_1', '36_2', '40_2', '54_2', '57_2', '60_1', '62_1'],\n",
    "               ['17_5', '20_3', '23_3', '24_2', '28_2', '30_2', '33_3', '51_2', '52_2', '59_2', '63_3', '66_2'], \n",
    "               ['12_1', '14_2', '22_3', '26_3', '35_4', '44_1', '45_1', '47_2', '49_1', '53_2', '56_2', '68_1'], \n",
    "               ['4_4', '5_3', '8_1', '10_3', '25_3', '27_1', '29_2', '37_1', '42_3', '48_3', '50_1', '69_1'], \n",
    "               ['7_2', '15_4', '19_2', '31_1', '43_1', '46_2', '55_2', '58_2', '61_1', '64_1', '65_1', '67_1', '70_1']]\n",
    "\n",
    "#Which pre-trained weight sets should be used for ResNet and DenseNet model: 'IMAGENET1K_V1', 'IMAGENET1K_V2'\n",
    "#Unclear which weights were used for TensorFlow ResNet50 variant in original implementation, but V2 did improve scores a bit when using resizeSize = 0\n",
    "weightsResNet = 'IMAGENET1K_V2'\n",
    "\n",
    "#Which pre-trained weight sets should be used for DenseNet model: 'IMAGENET1K_V1'\n",
    "weightsDenseNet = 'IMAGENET1K_V1'\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "#CLASSIFICATION - WSI\n",
    "#==================================================================\n",
    "#Should WSI preparation and block extraction overwrite previously generated files)\n",
    "overwrite_WSI_blocks = True\n",
    "\n",
    "#Should features be extracted for WSI extracted blocks and overwrite previously generated files\n",
    "overwrite_WSI_features = True\n",
    "\n",
    "#Should saliency maps be determined for WSI extracted blocks and overwrite previously generated files\n",
    "overwrite_WSI_saliencyMaps = True\n",
    "\n",
    "#Should the decision fusion mode be used for block classification (default: True)\n",
    "fusionMode_WSI = True\n",
    "\n",
    "#Should saliency maps and their overlays be visualized for WSI\n",
    "visualizeSaliencyMaps_WSI = True\n",
    "\n",
    "#Should prediction grids and their overlays be visualized for WSI\n",
    "visualizePredictionGrids_WSI = True\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "#CLASSIFICATION - Reconstruction\n",
    "#==================================================================\n",
    "#Should visuals of the reconstruction model input data be generated\n",
    "visualizeInputData_recon = True\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "#RARELY CHANGED\n",
    "#==================================================================\n",
    "\n",
    "#What is the camera resolution in mm/pixel for the instrument that acquired the data being used\n",
    "cameraResolution = 0.00454\n",
    "\n",
    "#What is the minimum area/quantity (in mm^2) of foreground data that should qualify a block for classification\n",
    "#Decrease for increased sensitivity and vice versa; result should not exceed blockSize*cameraResolution\n",
    "#As the classifier was not trained to handle blank background blocks, setting to low will harm performance\n",
    "minimumForegroundArea = 1.0**2\n",
    "\n",
    "#Minimum value [0, 255] for a grayscale pixel to be considered as a foreground location during block extraction (default: 11)\n",
    "#-1 will automatically determine a new value as the minimum Otsu threshold across all available WSI; default value from prior determination\n",
    "blockBackgroundValue = -1\n",
    "\n",
    "#When splitting WSI images, what size should the resulting blocks be (default: 400)\n",
    "#Should remain consistent with block sizes given for training\n",
    "blockSize = 400\n",
    "\n",
    "#Specify what symmetrical dimension blocks should be resized to; if no resizing is desired leave as 0 (default: 224)\n",
    "#Leaving as 0 will increase training time (also must change batchsizeClassifier), but can lead to improved scores\n",
    "#Original implementation uses 224, though with adaptive average pooling this isn't actually neccessary\n",
    "resizeSize_blocks = 224\n",
    "\n",
    "#Specify what symmetrical dimension WSI should be resized to when generating saliency maps (default: 224)\n",
    "#If fusion method were to be further developed, this should be changed to maintain the original sample aspect ratio. \n",
    "resizeSize_WSI = 224\n",
    "\n",
    "#How thick should the grid lines be when generating overlay images (defualt: 50)\n",
    "gridThickness = 50\n",
    "\n",
    "#Should saliency map overlays be placed over data converted to grayscale for clearer visualization (default: True)\n",
    "overlayGray = True\n",
    "\n",
    "#Should images with overlay data (and grid maps) be saved to lossless (.tif) or compressed (.jpg) image format (default: False)\n",
    "overlayLossless = False\n",
    "\n",
    "#For .jpg image outputs, what should the compression quality (%) be (default: 95)\n",
    "#WARNING: Setting to 100 is not sufficient to generate in lossless/exact outputs; if that is desired, use overlayLossless instead! \n",
    "exportQuality = 95\n",
    "\n",
    "#What weight should be used when overlaying data\n",
    "overlayWeight = 0.5\n",
    "\n",
    "#Define labels used for normal/benign tissue\n",
    "#'a': normal adipose.\n",
    "#'s': normal stroma tissue excluding adipose.\n",
    "#'o': other normal tissue including parenchyma, adenosis, lobules, blood vessels, etc.\n",
    "labelsBenign = ['a', 's', 'o', 'normal']\n",
    "\n",
    "#Define labels used for malignant tissue\n",
    "#'d': IDC tumor\n",
    "#'l': ILC tumor\n",
    "#'ot': other tumor areas including DCIS, biopsy site, and slightly defocused tumor regions.\n",
    "labelsMalignant = ['d', 'l', 'ot', 'tumor']\n",
    "\n",
    "#Define labels used for tissues to be excluded\n",
    "#'ft': defocused but still visually tumor-like areas.\n",
    "#'f': severly out-of-focusing areas. \n",
    "#'b': background. \n",
    "#'e': bubbles.\n",
    "labelsExclude = ['ft', 'f', 'b', 'e', 'exclude']\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "\n",
    "#Already in setup CONFIG (avoid duplication during copy)\n",
    "#==================================================================\n",
    "#Should parallelization calls be used to leverage multithreading where able\n",
    "parallelization = True\n",
    "\n",
    "#If parallelization is enabled, how many CPU threads should be used in Ray tasks? (0 will use any/all available)\n",
    "#Recommend starting at half of the available system threads if using hyperthreading,\n",
    "#or 1-2 less than the number of system CPU cores if not using hyperthreading.\n",
    "#Adjust to where the CPU just below 100% usage during parallel operations \n",
    "availableThreads = 16\n",
    "\n",
    "#Which GPU(s) devices should be used (last specified used for data processing and model training); (default: [-1], any/all available; CPU only: [])\n",
    "gpus = [-1]\n",
    "\n",
    "#Should training/validation data be entirely stored on GPU (default: True; improves training/validation efficiency, set to False if OOM occurs)\n",
    "storeOnDevice = True\n",
    "   \n",
    "#RNG seed value to ensure run-to-run consistency (-1 to disable)\n",
    "manualSeedValue = 0\n",
    "\n",
    "#Should warnings and info messages be shown during operation\n",
    "debugMode = True\n",
    "\n",
    "#Should progress bars be visualized with ascii characters\n",
    "asciiFlag = False\n",
    "\n",
    "#==================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#MAIN - Run codes that have already been completed\n",
    "#==================================================================\n",
    "\n",
    "exec(open(\"./CODE/EXTERNAL.py\", encoding='utf-8').read())\n",
    "\n",
    "exec(open(\"./CODE/COMPUTE.py\", encoding='utf-8').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2217c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#MODEL_CLASS\n",
    "#==================================================================\n",
    "\n",
    "#Load and preprocess data image files\n",
    "#ToTensor() swaps axes (so not used); convert to contiguous torch tensor manually\n",
    "#Rescaling and changing data type only after resizing (otherwise can escape [0, 1])\n",
    "class DataPreprocessing_Classifier(Dataset):\n",
    "    def __init__(self, filenames, resizeSize):\n",
    "        super().__init__()\n",
    "        self.filenames = filenames\n",
    "        self.numFiles = len(self.filenames)\n",
    "        if resizeSize > 0: self.transform = generateTransform([resizeSize, resizeSize], True, True)\n",
    "        else: self.transform = generateTransform([], True, True)\n",
    "        \n",
    "    #Rearranging import dimensions, allows resize transform before tensor-conversion/rescaling; preserves precision and output data range\n",
    "    def __getitem__(self, index): return self.transform(np.moveaxis(cv2.cvtColor(cv2.imread(self.filenames[index], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB), -1, 0))\n",
    "    \n",
    "    def __len__(self): return self.numFiles\n",
    "    \n",
    "#Load/synchronize data labeling, drop excluded rows, and extract relevant metadata\n",
    "def loadMetadata_blocks(filename):\n",
    "    metadata = pd.read_csv(filename, header=0, names=['Sample', 'Index', 'Row', 'Column', 'Label'], converters={'Sample':str,'Index':str, 'Row':int, 'Column':int, 'Label':str})\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsBenign, labelBenign)\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsMalignant, labelMalignant)\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsExclude, labelExclude)\n",
    "    metadata = metadata.loc[metadata['Label'] != labelExclude]\n",
    "    return [np.squeeze(data) for data in np.split(np.asarray(metadata), [1, 2, 4], -1)]\n",
    "\n",
    "#Load/synchronize data labeling, drop excluded rows, and extract relevant metadata\n",
    "def loadMetadata_WSI(filename):\n",
    "    metadata = pd.read_csv(filename, header=0, names=['Sample', 'Label'], converters={'Sample':str, 'Label':str})\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsBenign, labelBenign)\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsMalignant, labelMalignant)\n",
    "    metadata['Label'] = metadata['Label'].replace(labelsExclude, labelExclude)\n",
    "    metadata = metadata.loc[metadata['Label'] != labelExclude]\n",
    "    return [np.squeeze(data) for data in np.split(np.asarray(metadata), 2, -1)]\n",
    "\n",
    "#Extract blocks and determine associated metadata for referenced WSI files; abstraction allows for isolation of WSI data used for training the classifier\n",
    "def extractBlocks(WSIFilenames):\n",
    "    \n",
    "    #Extract uniform, non-overlapping blocks from each WSI; may be too memory intensive and RW-bottlenecked to parallelize efficiently\n",
    "    blockNames, blockFilenames, blockSampleNames, blockLocations, cropData, paddingData, shapeData = [], [], [], [], [], [], []\n",
    "    for filename in tqdm(WSIFilenames, desc='Block Extraction', leave=True, ascii=asciiFlag):\n",
    "        \n",
    "        #Extract base sample name\n",
    "        sampleName = os.path.basename(filename).split('.jpg')[0]\n",
    "        \n",
    "        #Load WSI image\n",
    "        imageWSI = cv2.cvtColor(cv2.imread(filename, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Tried a few quicker methods for pre-processing before applying some foreground segmentation\n",
    "        #No tangible benefits were observed in performance, but leaving in code at least once for archival reference\n",
    "        \n",
    "        #Histogram equalization\n",
    "        #y, cr, cb = cv2.split(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2YCrCb))\n",
    "        #imageMask = cv2.cvtColor(cv2.merge((cv2.equalizeHist(y), cr, cb)), cv2.COLOR_YCR_CB2RGB)\n",
    "        \n",
    "        #Denoising\n",
    "        #imageMask = cv2.fastNlMeansDenoising(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY), None, h=3, templateWindowSize=7, searchWindowSize=21)\n",
    "        \n",
    "        #CLAHE\n",
    "        #imageMask = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8)).apply(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY))\n",
    "        \n",
    "        #Isolate only the largest non-zero area; would not be able to handle samples with disconnected tissue segments\n",
    "        #imageMask = cv2.threshold(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY),0,255,cv2.THRESH_BINARY)[1]\n",
    "        #imageMask = max(cv2.findContours(imageMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0], key=cv2.contourArea)\n",
    "        \n",
    "        #Crop WSI to the foreground area using Otsu\n",
    "        imageWSI_gray = cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY)\n",
    "        x, y, w, h = cv2.boundingRect(cv2.threshold(imageWSI_gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]) \n",
    "        imageWSI = imageWSI[y:y+h, x:x+w]\n",
    "        imageWSI_gray = imageWSI_gray[y:y+h, x:x+w]\n",
    "        cropData.append([y, y+h, x, x+w])\n",
    "        \n",
    "        #Pad the image as needed (as symmetrially as possible) for an even division by the specified block size; compute numBlocks per row/column\n",
    "        padHeight = (int(np.ceil(imageWSI.shape[0]/blockSize))*blockSize)-imageWSI.shape[0]\n",
    "        padWidth = (int(np.ceil(imageWSI.shape[1]/blockSize))*blockSize)-imageWSI.shape[1]\n",
    "        padTop, padLeft = padHeight//2, padWidth//2\n",
    "        padBottom, padRight = padTop+(padHeight%2), padLeft+(padWidth%2)\n",
    "        imageWSI = np.pad(imageWSI, ((padTop, padBottom), (padLeft, padRight), (0, 0)))\n",
    "        imageWSI_gray = np.pad(imageWSI_gray, ((padTop, padBottom), (padLeft, padRight)))\n",
    "        paddingData.append([padTop, padBottom, padLeft, padRight])\n",
    "        numBlocksRow, numBlocksCol = imageWSI.shape[0]//blockSize, imageWSI.shape[1]//blockSize\n",
    "        shapeData.append([numBlocksRow, numBlocksCol])\n",
    "        \n",
    "        #Split the WSI (color and grayscale) into blocks and flatten\n",
    "        imageWSI = imageWSI.reshape(numBlocksRow, blockSize, numBlocksCol, blockSize, imageWSI.shape[2]).swapaxes(1,2)\n",
    "        imageWSI = imageWSI.reshape(-1, imageWSI.shape[2], imageWSI.shape[3], imageWSI.shape[4])\n",
    "        imageWSI_gray = imageWSI_gray.reshape(numBlocksRow, blockSize, numBlocksCol, blockSize).swapaxes(1,2)\n",
    "        imageWSI_gray = imageWSI_gray.reshape(-1, imageWSI_gray.shape[2], imageWSI_gray.shape[3])\n",
    "        \n",
    "        #Setup directory to store blocks\n",
    "        dir_WSI_sampleBlocks = dir_WSI_blocks + 'S' + sampleName + os.path.sep\n",
    "        if not os.path.exists(dir_WSI_sampleBlocks): os.makedirs(dir_WSI_sampleBlocks)\n",
    "        \n",
    "        #Record metadata for each block that has a specified percentage of foreground data and save each to disk\n",
    "        blockIndex = 0\n",
    "        for rowNum in range(0, numBlocksRow):\n",
    "            for colNum in range(0, numBlocksCol):\n",
    "                if np.mean(imageWSI_gray[blockIndex] >= blockBackgroundValue) >= blockBackgroundRatio: \n",
    "                    locationRow, locationColumn= rowNum*blockSize, colNum*blockSize\n",
    "                    blockLocations.append([locationRow, locationColumn])\n",
    "                    blockName = sampleName+'_'+str(blockIndex)+'_'+str(locationRow)+'_'+str(locationColumn)\n",
    "                    blockNames.append(blockName)\n",
    "                    blockSampleNames.append(sampleName)\n",
    "                    blockFilenames.append(dir_WSI_sampleBlocks+'PS'+blockName+'.tif')\n",
    "                    exportImage(blockFilenames[-1], imageWSI[blockIndex])\n",
    "                blockIndex += 1\n",
    "    \n",
    "    return np.asarray(blockNames), np.asarray(blockFilenames), np.asarray(blockSampleNames), np.asarray(blockLocations), np.asarray(cropData), np.asarray(paddingData), np.asarray(shapeData)\n",
    "\n",
    "#Compute metrics for a classification result and visualize/save them as needed\n",
    "def computeClassificationMetrics(labels, predictions, baseFilename):\n",
    "    \n",
    "    #Specify data class labels\n",
    "    displayLabels = ['Benign', 'Malignant']\n",
    "    classLabels = np.arange(0, len(displayLabels), 1)\n",
    "    \n",
    "    #Generate/store a classification report: precision, recall, f1-score, and support\n",
    "    classificationReport = classification_report(labels, predictions, labels=classLabels, target_names=displayLabels, output_dict=True)#, zero_division=0.0)\n",
    "    classificationReport = pd.DataFrame(classificationReport).transpose()\n",
    "    classificationReport.to_csv(baseFilename + '_classificationReport.csv')\n",
    "\n",
    "    #Generate a confusion matrix and extract relevant statistics\n",
    "    confusionMatrix = confusion_matrix(labels, predictions, labels=classLabels)\n",
    "    tn, fp, fn, tp = confusionMatrix.ravel()\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    \n",
    "    #Store relevant statistics\n",
    "    summaryReport = {'Accuracy': accuracy, 'Sensitivity': sensitivity, 'Specificity': specificity}\n",
    "    summaryReport = pd.DataFrame.from_dict(summaryReport, orient='index')\n",
    "    summaryReport.to_csv(baseFilename + '_summaryReport.csv')\n",
    "    \n",
    "    #Store confusion matrix\n",
    "    displayCM = ConfusionMatrixDisplay(confusionMatrix, display_labels=displayLabels)\n",
    "    displayCM.plot(cmap='Blues')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(baseFilename+'_confusionMatrix.tif')\n",
    "    plt.close()\n",
    "    \n",
    "#Generate a torch transform needed for preprocessing image data\n",
    "def generateTransform(resizeSize=[], rescale=False, normalize=False):\n",
    "    transform = [lambda inputs : torch.from_numpy(inputs).contiguous()]\n",
    "    if len(resizeSize) > 0: transform.append(v2.Resize(tuple(resizeSize)))\n",
    "    if rescale: transform.append(lambda inputs: inputs.to(dtype=torch.get_default_dtype()).div(255))\n",
    "    if normalize: transform.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "#Export lossless RGB image data to disk\n",
    "def exportImage(filename, image):\n",
    "    if filename.split('.')[-1] == 'tif': writeSuccess = cv2.imwrite(filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "    elif filename.split('.')[-1] == 'jpg': writeSuccess = cv2.imwrite(filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), exportQuality])\n",
    "    else: sys.exit('\\nError - Specified image output format has not been implemented.')\n",
    "    if not writeSuccess: sys.exit('\\nError - Unable to write file: ', filename)\n",
    "\n",
    "#OpenCV does not output sharp corners with its rectangle method...unless it's filled in\n",
    "def rectangle(image, startPos, endPos, color):\n",
    "    image = cv2.rectangle(image, startPos, endPos, color, -1)\n",
    "    image = cv2.rectangle(image, (startPos[0]+gridThicknessOffset, startPos[1]+gridThicknessOffset), (endPos[0]-gridThicknessOffset, endPos[1]-gridThicknessOffset), (0, 0, 0), -1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4333a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#INTERNAL_DIRECTORY\n",
    "#==================================================================\n",
    "\n",
    "#Indicate and setup the destination folder for results of this configuration\n",
    "destResultsFolder = './RESULTS_'+os.path.splitext(os.path.basename(configFileName).split('_')[1])[0]\n",
    "\n",
    "#If the folder already exists, either remove it, or append a novel value to it\n",
    "if os.path.exists(destResultsFolder):\n",
    "    if not preventResultsOverwrite: \n",
    "        shutil.rmtree(destResultsFolder)\n",
    "    else: \n",
    "        destinationNameValue = 0\n",
    "        destResultsFolder_Base = copy.deepcopy(destResultsFolder)\n",
    "        while True:\n",
    "            destResultsFolder = destResultsFolder_Base + '_' + str(destinationNameValue)\n",
    "            if not os.path.exists(destResultsFolder): break\n",
    "            destinationNameValue += 1\n",
    "\n",
    "#Data input directories and file locations\n",
    "#=============================================================================\n",
    "#Global\n",
    "dir_data = '.' + os.path.sep + 'DATA' + os.path.sep\n",
    "dir_results = '.' + os.path.sep + 'RESULTS' + os.path.sep\n",
    "dir_classifier_models = dir_results + 'MODELS' + os.path.sep\n",
    "\n",
    "#Block classification\n",
    "dir_blocks_data = dir_data + 'BLOCKS' + os.path.sep\n",
    "dir_blocks_inputBlocks = dir_blocks_data + 'INPUT_BLOCKS' + os.path.sep\n",
    "file_blocks_metadataBlocks = dir_blocks_inputBlocks + 'metadata_blocks.csv'\n",
    "dir_blocks_inputWSI = dir_blocks_data + 'INPUT_WSI' + os.path.sep\n",
    "file_blocks_metadataWSI = dir_blocks_inputWSI + 'metadata_WSI.csv'\n",
    "\n",
    "dir_blocks_features = dir_blocks_data + 'OUTPUT_FEATURES' + os.path.sep\n",
    "dir_blocks_salicencyMaps = dir_blocks_data + 'OUTPUT_SALIENCY_MAPS' + os.path.sep\n",
    "\n",
    "dir_blocks_visuals = dir_blocks_data + 'OUTPUT_VISUALS' + os.path.sep\n",
    "dir_blocks_visuals_saliencyMaps = dir_blocks_visuals + 'SALIENCY_MAPS' + os.path.sep\n",
    "dir_blocks_visuals_overlaidSaliencyMaps = dir_blocks_visuals + 'OVERLAID_SALICENCY_MAPS' + os.path.sep\n",
    "dir_blocks_visuals_labelGrids = dir_blocks_visuals + 'LABEL_GRIDS' + os.path.sep\n",
    "dir_blocks_visuals_overlaidLabelGrids = dir_blocks_visuals + 'OVERLAID_LABEL_GRIDS' + os.path.sep\n",
    "\n",
    "dir_blocks_results = dir_results + 'BLOCKS' + os.path.sep\n",
    "dir_blocks_results_labelGrids = dir_blocks_results + 'LABEL_GRIDS' + os.path.sep\n",
    "dir_blocks_results_overlaidLabelGrids = dir_blocks_results + 'OVERLAID_LABEL_GRIDS' + os.path.sep\n",
    "\n",
    "\n",
    "#WSI classification\n",
    "dir_WSI_data = dir_data + 'WSI' + os.path.sep\n",
    "dir_WSI_inputs = dir_WSI_data + 'INPUT_WSI' + os.path.sep\n",
    "file_WSI_metadataWSI = dir_WSI_inputs + 'metadata_WSI.csv'\n",
    "\n",
    "dir_WSI_blocks = dir_WSI_data + 'OUTPUT_BLOCKS' + os.path.sep\n",
    "dir_WSI_features = dir_WSI_data + 'OUTPUT_FEATURES' + os.path.sep\n",
    "dir_WSI_saliencyMaps = dir_WSI_data + 'OUTPUT_SALIENCY_MAPS' + os.path.sep\n",
    "\n",
    "dir_WSI_visuals = dir_WSI_data + 'OUTPUT_VISUALS' + os.path.sep\n",
    "dir_WSI_visuals_saliencyMaps = dir_WSI_visuals + 'SALIENCY_MAPS' + os.path.sep\n",
    "dir_WSI_visuals_overlaidSaliencyMaps = dir_WSI_visuals + 'OVERLAID_SALIENCY_MAPS' + os.path.sep\n",
    "\n",
    "dir_WSI_results = dir_results + 'WSI' + os.path.sep\n",
    "dir_WSI_results_labelGrids = dir_WSI_results + 'LABEL_GRIDS' + os.path.sep\n",
    "dir_WSI_results_overlaidLabelGrids = dir_WSI_results + 'OVERLAID_LABEL_GRIDS' + os.path.sep\n",
    "\n",
    "#Reconstruction model input data classification\n",
    "dir_recon_data = dir_data + 'RECON' + os.path.sep\n",
    "dir_recon_inputData = dir_recon_data + 'INPUT_RECON' + os.path.sep\n",
    "dir_recon_visuals_inputData = dir_recon_data + 'INPUT_VISUALS' + os.path.sep\n",
    "\n",
    "dir_recon_results = dir_results + 'RECON' + os.path.sep\n",
    "dir_recon_results_train = dir_recon_results + 'TRAIN' + os.path.sep\n",
    "dir_recon_results_test = dir_recon_results + 'TEST' + os.path.sep\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "#If folders do not exist, but their use is enabled, exit the program\n",
    "#=============================================================================\n",
    "#Block classification\n",
    "if not os.path.exists(dir_data): sys.exit('\\nError - Required folder: ' + dir_data + ' does not exist.')\n",
    "if not os.path.exists(dir_blocks_data) and (classifierBlocks or classifierExport): sys.exit('\\nError - Required folder: ' + dir_blocks_data + ' does not exist.')\n",
    "if not os.path.exists(dir_blocks_inputBlocks) and (classifierBlocks or classifierExport): sys.exit('\\nError - Required folder: ' + dir_blocks_inputBlocks + ' does not exist.')\n",
    "if not os.path.exists(dir_blocks_inputWSI) and (classifierBlocks or classifierExport): sys.exit('\\nError - Required folder: ' + dir_blocks_inputWSI + ' does not exist.')\n",
    "\n",
    "#WSI classification\n",
    "if not os.path.exists(dir_WSI_data) and classifierWSI: sys.exit('\\nError - Required folder: ' + dir_WSI_data + ' does not exist.')\n",
    "if not os.path.exists(dir_WSI_inputs) and classifierWSI: sys.exit('\\nError - Required folder: ' + dir_WSI_inputs + ' does not exist.')\n",
    "\n",
    "#Reconstruction model input data classification\n",
    "if (len(glob.glob(dir_WSI_features+'*.npy'))!=2 or len(glob.glob(dir_WSI_saliencyMaps+'*.npy'))!=2) and (classifierRecon and not classifierWSI): sys.exit('\\Error - WSI block features/weights, required for generation of reconstruction model input data are not available.')\n",
    "\n",
    "#=============================================================================\n",
    "#If a task is diabled, then overwrites should be disabled to prevent overwrite of existing data\n",
    "#If for a given task, file overwrites are not enabled, and the needed files do not exist, then enable the relevant overwrite(s)\n",
    "#=============================================================================\n",
    "\n",
    "if (classifierBlocks or classifierExport):\n",
    "    if not overwrite_blocks_features and len(glob.glob(dir_blocks_features+'*.npy'))==0: overwrite_blocks_features = True    \n",
    "\n",
    "if classifierBlocks:\n",
    "    if not overwrite_blocks_saliencyMaps and fusionMode_blocks and len(glob.glob(dir_blocks_salicencyMaps+'*.npy'))==0: overwrite_blocks_saliencyMaps = True\n",
    "else:\n",
    "    overwrite_blocks_features = False\n",
    "    overwrite_blocks_saliencyMaps = False\n",
    "\n",
    "if classifierWSI:\n",
    "    if not overwrite_WSI_blocks and len(glob.glob(dir_WSI_blocks+'*.npy'))==0: overwrite_WSI_blocks = True\n",
    "    if not overwrite_WSI_features and len(glob.glob(dir_WSI_features+'*.npy'))==0: overwrite_WSI_features = True\n",
    "    if not overwrite_WSI_saliencyMaps and fusionMode_WSI and len(glob.glob(dir_WSI_saliencyMaps+'*.npy'))==0: overwrite_WSI_saliencyMaps = True\n",
    "else:\n",
    "    overwrite_WSI_blocks = False\n",
    "    overwrite_WSI_features = False\n",
    "    overwrite_WSI_saliencyMaps = False\n",
    "    \n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "#Regenerate empty files/folders that are to be overwritten\n",
    "#=============================================================================\n",
    "#Result folders\n",
    "if not os.path.exists(dir_results): os.makedirs(dir_results)\n",
    "if classifierExport and os.path.exists(dir_classifier_models): shutil.rmtree(dir_classifier_models)\n",
    "if classifierBlocks and os.path.exists(dir_blocks_results): shutil.rmtree(dir_blocks_results)\n",
    "if classifierWSI and os.path.exists(dir_WSI_results): shutil.rmtree(dir_WSI_results)\n",
    "if classifierRecon and os.path.exists(dir_recon_results): shutil.rmtree(dir_recon_results)\n",
    "checkDirectories = [dir_classifier_models,\n",
    "                    dir_blocks_results,\n",
    "                    dir_blocks_results_labelGrids,\n",
    "                    dir_blocks_results_overlaidLabelGrids, \n",
    "                    dir_WSI_results, \n",
    "                    dir_WSI_results_labelGrids,\n",
    "                    dir_WSI_results_overlaidLabelGrids,\n",
    "                    dir_recon_results,\n",
    "                    dir_recon_results_train,\n",
    "                    dir_recon_results_test\n",
    "                   ]\n",
    "for directory in checkDirectories: os.makedirs(directory)\n",
    "    \n",
    "#Block classification\n",
    "if overwrite_blocks_features and os.path.exists(dir_blocks_features): shutil.rmtree(dir_blocks_features)\n",
    "if overwrite_blocks_saliencyMaps: \n",
    "    if os.path.exists(dir_blocks_salicencyMaps): shutil.rmtree(dir_blocks_salicencyMaps)\n",
    "    if os.path.exists(dir_blocks_visuals_saliencyMaps): shutil.rmtree(dir_blocks_visuals_saliencyMaps)\n",
    "    if os.path.exists(dir_blocks_visuals_overlaidSaliencyMaps): shutil.rmtree(dir_blocks_visuals_overlaidSaliencyMaps)\n",
    "if visualizeLabelGrids_blocks:\n",
    "    if os.path.exists(dir_blocks_visuals_labelGrids): shutil.rmtree(dir_blocks_visuals_labelGrids)\n",
    "    if os.path.exists(dir_blocks_visuals_overlaidLabelGrids): shutil.rmtree(dir_blocks_visuals_overlaidLabelGrids)\n",
    "    \n",
    "#WSI classification\n",
    "if overwrite_WSI_blocks and os.path.exists(dir_WSI_blocks): shutil.rmtree(dir_WSI_blocks)\n",
    "if overwrite_WSI_features and os.path.exists(dir_WSI_features): shutil.rmtree(dir_WSI_features)\n",
    "if overwrite_WSI_saliencyMaps: \n",
    "    if os.path.exists(dir_WSI_saliencyMaps): shutil.rmtree(dir_WSI_saliencyMaps)\n",
    "    if os.path.exists(dir_WSI_visuals_saliencyMaps): shutil.rmtree(dir_WSI_visuals_saliencyMaps)\n",
    "    if os.path.exists(dir_WSI_visuals_overlaidSaliencyMaps): shutil.rmtree(dir_WSI_visuals_overlaidSaliencyMaps)\n",
    "\n",
    "#Reconstruction model input data classification\n",
    "if classifierRecon:\n",
    "    if os.path.exists(dir_recon_data): shutil.rmtree(dir_recon_data)\n",
    "if visualizeInputData_recon:\n",
    "    if os.path.exists(dir_recon_visuals_inputData): shutil.rmtree(dir_recon_visuals_inputData)\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "#If data output (not result) folders do not exist, then create them\n",
    "#=============================================================================\n",
    "#Block classification\n",
    "checkDirectories = [dir_blocks_features, \n",
    "                    dir_blocks_salicencyMaps,\n",
    "                    dir_blocks_visuals,\n",
    "                    dir_blocks_visuals_saliencyMaps,\n",
    "                    dir_blocks_visuals_overlaidSaliencyMaps,\n",
    "                    dir_blocks_visuals_labelGrids, \n",
    "                    dir_blocks_visuals_overlaidLabelGrids\n",
    "                   ]\n",
    "for directory in checkDirectories: \n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "\n",
    "#WSI classification\n",
    "checkDirectories = [dir_WSI_blocks, \n",
    "                    dir_WSI_features,\n",
    "                    dir_WSI_saliencyMaps,\n",
    "                    dir_WSI_visuals, \n",
    "                    dir_WSI_visuals_saliencyMaps,\n",
    "                    dir_WSI_visuals_overlaidSaliencyMaps,\n",
    "                   ]\n",
    "for directory in checkDirectories: \n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "        \n",
    "#Reconstruction model input data classification \n",
    "checkDirectories = [dir_recon_data, \n",
    "                    dir_recon_inputData,\n",
    "                    dir_recon_visuals_inputData\n",
    "                   ]\n",
    "for directory in checkDirectories: \n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "\n",
    "#=============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b36451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify image files\n",
    "class Classifier():\n",
    "    \n",
    "    #Load blocks specified by provided filenames, extract relevant features, and setup additional model components needed for training/evaluation\n",
    "    def __init__(self, dataType, blockNames, blockFilenames, blockSampleNames, blockLocations, sampleNames, WSIFilenames, WSILabels, cropData=[], paddingData=[], shapeData=[], blockLabels=None):\n",
    "        \n",
    "        #Store input variables internally\n",
    "        self.dataType = dataType\n",
    "        self.blockNames = blockNames\n",
    "        self.blockFilenames = blockFilenames\n",
    "        self.blockSampleNames = blockSampleNames\n",
    "        self.blockLocations = blockLocations\n",
    "        self.sampleNames = sampleNames\n",
    "        self.WSIFilenames = WSIFilenames\n",
    "        self.WSILabels = WSILabels\n",
    "        self.cropData = cropData\n",
    "        self.paddingData = paddingData\n",
    "        self.shapeData = shapeData\n",
    "        self.blockLabels = blockLabels\n",
    "        \n",
    "        #Specify internal object directories/data according to data type\n",
    "        if dataType == 'blocks':\n",
    "            self.dir_results = dir_blocks_results\n",
    "            self.dir_features = dir_blocks_features\n",
    "            self.dir_saliencyMaps = dir_blocks_salicencyMaps\n",
    "            self.dir_visuals_saliencyMaps = dir_blocks_visuals_saliencyMaps\n",
    "            self.dir_visuals_overlaidSaliencyMaps = dir_blocks_visuals_overlaidSaliencyMaps\n",
    "            self.dir_visuals_labelGrids = dir_blocks_visuals_labelGrids\n",
    "            self.dir_visuals_overlaidLabelGrids = dir_blocks_visuals_overlaidLabelGrids\n",
    "            self.dir_results_labelGrids = dir_blocks_results_labelGrids\n",
    "            self.dir_results_overlaidLabelGrids = dir_blocks_results_overlaidLabelGrids\n",
    "            self.visualizeLabelGrids = visualizeLabelGrids_blocks\n",
    "            self.visualizePredictionGrids = visualizePredictionGrids_blocks\n",
    "        elif dataType == 'WSI' or dataType == 'recon':\n",
    "            self.dir_results = dir_WSI_results\n",
    "            self.dir_features = dir_WSI_features\n",
    "            self.dir_saliencyMaps = dir_WSI_saliencyMaps\n",
    "            self.dir_visuals_saliencyMaps = dir_WSI_visuals_saliencyMaps\n",
    "            self.dir_visuals_overlaidSaliencyMaps = dir_WSI_visuals_overlaidSaliencyMaps\n",
    "            self.dir_results_labelGrids = dir_WSI_results_labelGrids\n",
    "            self.dir_results_overlaidLabelGrids = dir_WSI_results_overlaidLabelGrids\n",
    "            self.visualizeLabelGrids = False\n",
    "            self.visualizePredictionGrids = visualizePredictionGrids_WSI\n",
    "        else:\n",
    "            sys.error('\\nError - Unknown data type used when creating classifier object.')\n",
    "            \n",
    "        #If reconstruction model input data is being generated, then load features and weights of applicable blocks\n",
    "        if dataType == 'recon':\n",
    "            blockFeatures_WSI_blocks = np.load(self.dir_features + 'blockFeatures_WSI_blocks.npy', allow_pickle=True)\n",
    "            blockFeatures_WSI_WSI = np.load(self.dir_features + 'blockFeatures_WSI_WSI.npy', allow_pickle=True)\n",
    "            self.blockFeatures = np.concatenate([blockFeatures_WSI_blocks, blockFeatures_WSI_WSI])\n",
    "            np.save(dir_recon_inputData + 'blockFeatures', self.blockFeatures)\n",
    "            del blockFeatures_WSI_WSI, blockFeatures_WSI_blocks\n",
    "            if fusionMode_WSI:\n",
    "                blockWeights_WSI_blocks = np.load(self.dir_saliencyMaps + 'blockWeights_WSI_blocks.npy', allow_pickle=True)\n",
    "                blockWeights_WSI_WSI = np.load(self.dir_saliencyMaps + 'blockWeights_WSI_WSI.npy', allow_pickle=True)\n",
    "                self.blockWeights = np.concatenate([blockWeights_WSI_blocks, blockWeights_WSI_WSI])\n",
    "                np.save(dir_recon_inputData + 'blockWeights', self.blockWeights)\n",
    "                del blockWeights_WSI_WSI, blockWeights_WSI_blocks\n",
    "        \n",
    "        #Prepare data objects for obtaining/processing PyTorch model inputs\n",
    "        #Using num_workers>1 in the DataLoader objects causes bizzare semaphore/lock/descriptor issues/warnings; still appears to work, but keeping to 1 for safety\n",
    "        self.device = f\"cuda:{gpus[-1]}\" if len(gpus) > 0 else \"cpu\"\n",
    "        self.torchDevice = torch.device(self.device)\n",
    "        self.blockData = DataPreprocessing_Classifier(blockFilenames, resizeSize_blocks)\n",
    "        self.blockDataloader = DataLoader(self.blockData, batch_size=batchsizeClassifier, num_workers=1, shuffle=False, pin_memory=True)\n",
    "        self.numBlockData = len(self.blockDataloader)\n",
    "        self.WSIData = DataPreprocessing_Classifier(WSIFilenames, resizeSize_WSI)\n",
    "        self.WSIDataloader = DataLoader(self.WSIData, batch_size=batchsizeClassifier, num_workers=1, shuffle=False, pin_memory=True)\n",
    "        self.numWSIData = len(self.WSIDataloader)\n",
    "        \n",
    "        #Set default cuda device for XGBClassifier input data\n",
    "        if len(gpus) > 0: cp.cuda.Device(gpus[-1]).use()\n",
    "        \n",
    "    def computeFeatures(self, overwrite_Features, suffix=''):\n",
    "        \n",
    "        #Extract or load features for the indicated block files\n",
    "        if overwrite_Features: \n",
    "        \n",
    "            #Load pretrained ResNet50 model and set to evaluation mode\n",
    "            model_ResNet = models.resnet50(weights=weightsResNet).to(self.torchDevice)\n",
    "            _ = model_ResNet.train(False)\n",
    "\n",
    "            #Extract features for each batch of sample block images\n",
    "            self.blockFeatures = []\n",
    "            for data in tqdm(self.blockDataloader, total=self.numBlockData, desc='Feature Extraction', leave=True, ascii=asciiFlag):\n",
    "                self.blockFeatures += model_ResNet(data.to(self.torchDevice)).detach().cpu().tolist()\n",
    "            \n",
    "            #Clear the ResNet model\n",
    "            del model_ResNet\n",
    "            if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "            \n",
    "            #Convert list of features to an array\n",
    "            self.blockFeatures = np.asarray(self.blockFeatures)\n",
    "            \n",
    "            #Save features to disk\n",
    "            np.save(self.dir_features + 'blockFeatures'+suffix, self.blockFeatures)\n",
    "            \n",
    "        else: \n",
    "            self.blockFeatures = np.load(self.dir_features + 'blockFeatures'+suffix+'.npy', allow_pickle=True)\n",
    "        \n",
    "    def computeSalicencyMaps(self, overwrite_saliencyMaps, visualizeSaliencyMaps, suffix=''):\n",
    "        \n",
    "        #Extract or load saliency map data for the indicated block files\n",
    "        if overwrite_saliencyMaps:\n",
    "            \n",
    "            #Load pre-trained DenseNet\n",
    "            model_DenseNet = models.densenet169(weights=weightsDenseNet)\n",
    "\n",
    "            #Replace the in-built classifier; unclear how this structure and these hyperparameters were determined\n",
    "            model_DenseNet.classifier = nn.Sequential(\n",
    "                nn.Linear(model_DenseNet.classifier.in_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(256, 2),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "            model_DenseNet = model_DenseNet.to(self.torchDevice)\n",
    "            _ = model_DenseNet.train(False)\n",
    "\n",
    "            #Create GradCAMPlusPlus model; see https://github.com/jacobgil/pytorch-grad-cam for additional models and options\n",
    "            model_GradCamPlusPlus = GradCAMPlusPlus(model=model_DenseNet, target_layers=[model_DenseNet.features[-1]])\n",
    "\n",
    "            #Extract features for each batch of sample block images\n",
    "            #For current GradCamPlusPlus implementation, must manually clear internal copy of the outputs from GPU cache to prevent OOM\n",
    "            #Do not need to move data to device here, as managed by GradCAMPlusPlus (having already been placed on device)\n",
    "            self.saliencyMaps = []\n",
    "            for data in tqdm(self.WSIDataloader, total=self.numWSIData, desc='Computing Saliency Maps', leave=True, ascii=asciiFlag):\n",
    "                self.saliencyMaps.append(model_GradCamPlusPlus(input_tensor=data, targets=None))\n",
    "                del model_GradCamPlusPlus.outputs\n",
    "                if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "            self.saliencyMaps = np.vstack(self.saliencyMaps)\n",
    "            \n",
    "            #Clear DenseNet and GradCamPlusPlus\n",
    "            del model_DenseNet, model_GradCamPlusPlus\n",
    "            if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "            \n",
    "            #Process each of the resulting maps\n",
    "            self.blockWeights = []\n",
    "            for index, saliencyMap in tqdm(enumerate(self.saliencyMaps), total=len(self.saliencyMaps), desc='Processing Saliency Maps', leave=True, ascii=asciiFlag): \n",
    "                \n",
    "                #Load the sample WSI\n",
    "                imageWSI = cv2.cvtColor(cv2.imread(self.WSIFilenames[index], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                #If visualizations are enabled\n",
    "                if visualizeSaliencyMaps:\n",
    "                    \n",
    "                    #Store the saliency map to disk (keep at original output dimensions)\n",
    "                    exportImage(self.dir_visuals_saliencyMaps+'saliencyMap_'+self.sampleNames[index]+'.tif', matplotlib.cm.jet(saliencyMap)[:,:,:-1].astype(np.float32))\n",
    "                \n",
    "                    #Resize the saliency map to match the WSI dimensions\n",
    "                    transform = generateTransform(imageWSI.shape[:2], False, False)\n",
    "                    saliencyMap = transform(np.expand_dims(saliencyMap, 0))[0].numpy()\n",
    "                    \n",
    "                    #Overlay the saliency map on the WSI and save it to disk\n",
    "                    transform = generateTransform([], True, False)\n",
    "                    overlaid = np.moveaxis(transform(np.moveaxis(imageWSI, -1, 0)).numpy(), 0, -1)\n",
    "                    if overlayGray: overlaid = np.expand_dims(cv2.cvtColor(overlaid, cv2.COLOR_RGB2GRAY), -1)\n",
    "                    overlaid = show_cam_on_image(overlaid, saliencyMap, use_rgb=True, colormap=cv2.COLORMAP_HOT, image_weight=1.0-overlayWeight)\n",
    "                    exportImage(self.dir_visuals_overlaidSaliencyMaps+'overlaidSaliency_'+self.sampleNames[index]+overlayExtension, overlaid)\n",
    "                \n",
    "                #Extract saliency map data specific to sample block locations, compute regional importance as the average value, and threshold to get weights\n",
    "                blockWeights = []\n",
    "                for locationIndex, locationData in enumerate(self.blockLocations[np.where(self.blockSampleNames == self.sampleNames[index])[0]]):\n",
    "                    startRow, startColumn = locationData\n",
    "                    blockSaliencyMap = saliencyMap[startRow:startRow+blockSize, startColumn:startColumn+blockSize]\n",
    "                    blockImportance = np.mean(blockSaliencyMap)\n",
    "                    if blockImportance < 0.25: blockWeights.append(0)\n",
    "                    else: blockWeights.append(blockImportance)\n",
    "                self.blockWeights += blockWeights\n",
    "                \n",
    "            #Convert list of block weights to an array and save to disk\n",
    "            self.blockWeights = np.asarray(self.blockWeights)\n",
    "            np.save(self.dir_saliencyMaps + 'blockWeights'+suffix, self.blockWeights)\n",
    "            \n",
    "        else:\n",
    "            self.blockWeights = np.load(self.dir_saliencyMaps + 'blockWeights'+suffix+'.npy', allow_pickle=True)\n",
    "    \n",
    "    #Classify extracted block features\n",
    "    def predict(self, inputs, fusionMode, weights=None):\n",
    "        \n",
    "        #Compute the raw block predictions\n",
    "        predictions = self.model_XGBClassifier.predict(inputs.astype(np.float32))\n",
    "        \n",
    "        #If fusion mode is active, multiply the block predictions (using -1 for benign and +1 for malignant) by the matching weights; positive results are malignant\n",
    "        if fusionMode: \n",
    "            predictionsFusion = np.where(predictions==0, -1, 1)*weights\n",
    "            predictionsFusion = np.where(predictionsFusion>0, 1, 0)\n",
    "            return predictions.tolist(), predictionsFusion.tolist()\n",
    "        else: \n",
    "            return predictions.tolist()\n",
    "    \n",
    "    #Perform cross-validation\n",
    "    def crossValidation(self):\n",
    "        \n",
    "        #If block weights are available, then enable evaluation of fusion mode\n",
    "        if len(self.blockWeights)>0: fusionMode = True\n",
    "        else: fusionMode = False\n",
    "        \n",
    "        #Allocate samples to folds for cross validation\n",
    "        if type(manualFolds) != list: folds = [array.tolist() for array in np.array_split(np.random.permutation(self.sampleNames), manualFolds)]\n",
    "        else: folds = manualFolds\n",
    "        numFolds = len(folds)\n",
    "\n",
    "        #Split block features into specified folds, keeping track of originating indices and matched labels\n",
    "        foldsFeatures, foldsLabels, foldsWeights, foldsBlockSampleNames, foldsBlockNames, foldsBlockLocations, foldsWSILabels = [], [], [], [], [], [], []\n",
    "        for fold in folds:\n",
    "            blockIndices = np.concatenate([np.where(self.blockSampleNames == sampleName)[0] for sampleName in fold])\n",
    "            foldsBlockLocations.append(list(self.blockLocations[blockIndices]))\n",
    "            foldsFeatures.append(list(self.blockFeatures[blockIndices]))\n",
    "            foldsLabels.append(list(self.blockLabels[blockIndices]))\n",
    "            if fusionMode: foldsWeights.append(list(self.blockWeights[blockIndices]))\n",
    "            foldsBlockSampleNames.append(list(self.blockSampleNames[blockIndices]))\n",
    "            foldsBlockNames.append(list(self.blockNames[blockIndices]))            \n",
    "            foldsWSILabels += [self.WSILabels[np.where(self.sampleNames == sampleName)[0]][0] for sampleName in fold]\n",
    "        \n",
    "        #Collapse data for later (correct/matched ordered) evaluation of the fold data\n",
    "        foldsSampleNames = np.asarray(sum(folds, []))\n",
    "        foldsBlockLocations = np.concatenate(foldsBlockLocations)\n",
    "        foldsBlockSampleNames = np.concatenate(foldsBlockSampleNames)\n",
    "        foldsBlockNames = np.concatenate(foldsBlockNames)\n",
    "        foldsWSILabels = np.asarray(foldsWSILabels)\n",
    "        foldsWSIFilenames = np.concatenate([self.WSIFilenames[np.where(self.sampleNames == sampleName)[0]] for sampleName in foldsSampleNames])\n",
    "        \n",
    "        #Check class distribution between the folds\n",
    "        #print('B\\t M \\t Total')\n",
    "        #for foldNum in range(0, numFolds):\n",
    "        #    print(np.sum(np.array(foldsLabels[foldNum]) == valueBenign),'\\t', np.sum(np.array(foldsLabels[foldNum]) == valueMalignant), '\\t', len(foldsLabels[foldNum]))\n",
    "        \n",
    "        #Perform training/testing among the folds, testing sequentially (to ensure the correct order) and storing results for later evaluation\n",
    "        foldsBlockPredictions, foldsBlockPredictionsFusion = [], []\n",
    "        for foldNum in tqdm(range(0, numFolds), desc='Block Classification', leave=True, ascii=asciiFlag):\n",
    "            \n",
    "            #Train on all folds except the one specified\n",
    "            self.train(np.concatenate(foldsFeatures[:foldNum]+foldsFeatures[foldNum+1:]), np.concatenate(foldsLabels[:foldNum]+foldsLabels[foldNum+1:]))\n",
    "            \n",
    "            #Extract the testing data and place on the GPU if able\n",
    "            dataInput = np.asarray(foldsFeatures[foldNum])\n",
    "            if len(gpus) > 0: dataInput = cp.asarray(dataInput)\n",
    "            \n",
    "            #Classify blocks in the specified, remaining fold\n",
    "            if fusionMode: \n",
    "                predictions, predictionsFusion = self.predict(dataInput, fusionMode, np.asarray(foldsWeights[foldNum]))\n",
    "                foldsBlockPredictionsFusion += predictionsFusion\n",
    "            else:\n",
    "                predictions = self.predict(dataInput, fusionMode)\n",
    "            foldsBlockPredictions += predictions\n",
    "            \n",
    "            #Clear the XGBClassifier model and data on GPU\n",
    "            del self.model_XGBClassifier, dataInput\n",
    "            if len(gpus) > 0: \n",
    "                torch.cuda.empty_cache() \n",
    "                cp._default_memory_pool.free_all_blocks()\n",
    "        \n",
    "        #Convert lists of predictions to arrays and collapse labels for evaluation\n",
    "        foldsLabels = np.concatenate(foldsLabels)\n",
    "        foldsBlockPredictions = np.asarray(foldsBlockPredictions)\n",
    "        foldsBlockPredictionsFusion = np.asarray(foldsBlockPredictionsFusion)\n",
    "        \n",
    "        #Save results to disk\n",
    "        dataPrintout, dataPrintoutNames = [foldsBlockNames, foldsLabels, foldsBlockPredictions], ['Names', 'Labels', 'Raw Predictions']\n",
    "        if fusionMode: \n",
    "            dataPrintout.append(foldsBlockPredictionsFusion)\n",
    "            dataPrintoutNames.append('Fusion Predictions')\n",
    "        dataPrintout = pd.DataFrame(np.asarray(dataPrintout)).transpose()\n",
    "        dataPrintout.columns=dataPrintoutNames\n",
    "        dataPrintout.to_csv(self.dir_results + 'predictions_blocks.csv', index=False)\n",
    "        \n",
    "        #Evaluate per-block results\n",
    "        computeClassificationMetrics(foldsLabels, foldsBlockPredictions, self.dir_results+'results_blocks_initial')\n",
    "        if fusionMode: computeClassificationMetrics(foldsLabels, foldsBlockPredictionsFusion, self.dir_results+'results_blocks_fusion')\n",
    "    \n",
    "        #Classify each WSI, that had its component blocks classified, according to specified threshold of allowable malignant blocks\n",
    "        foldsSampleLabels, foldsSamplePredictions, foldsSamplePredictionsFusion = [], [], []\n",
    "        for foldsSampleIndex, sampleName in enumerate(foldsSampleNames):\n",
    "            blockIndices = np.where(foldsBlockSampleNames == sampleName)[0]\n",
    "            if thresholdWSI_GT: foldsSampleLabels.append((np.mean(foldsLabels[blockIndices]) >= thresholdWSI)*1)\n",
    "            foldsSamplePredictions.append((np.mean(foldsBlockPredictions[blockIndices]) >= thresholdWSI)*1)\n",
    "            if fusionMode: foldsSamplePredictionsFusion.append((np.mean(foldsBlockPredictionsFusion[blockIndices]) >= thresholdWSI)*1)\n",
    "        \n",
    "        #Convert list of WSI labels/predictions to arrays\n",
    "        if thresholdWSI_GT: \n",
    "            foldsSampleLabels = np.asarray(foldsSampleLabels)\n",
    "            mismatchedSamples = foldsSampleNames[np.where(foldsSampleLabels-foldsWSILabels != 0)[0]].tolist()\n",
    "            if len(mismatchedSamples) > 0: print('\\nWarning - Determination of ground-truth labels for WSI using threshold method did not match with actual WSI labels for the following samples:\\n' + str(mismatchedSamples)+'\\nStrongly advise revising WSI threshold criteria or disabling thresholdWSI_GT.')\n",
    "        else: foldsSampleLabels = foldsWSILabels\n",
    "        foldsSamplePredictions = np.asarray(foldsSamplePredictions)\n",
    "        foldsSamplePredictionsFusion = np.asarray(foldsSamplePredictionsFusion)\n",
    "        \n",
    "        #Evaluate per-sample results\n",
    "        self.evaluateResultsWSI(foldsSampleNames, foldsWSIFilenames, foldsSampleLabels, foldsSamplePredictions, foldsSamplePredictionsFusion, foldsBlockSampleNames, foldsLabels, foldsBlockPredictions, foldsBlockPredictionsFusion, foldsBlockLocations)\n",
    "\n",
    "    def evaluateResultsWSI(self, sampleNames, WSIFilenames, sampleLabels, samplePredictions, samplePredictionsFusion, blockSampleNames, blockLabels, blockPredictions, blockPredictionsFusion, blockLocations):\n",
    "        \n",
    "        #If block weights are available, then enable evaluation of fusion mode\n",
    "        if len(self.blockWeights)>0: fusionMode = True\n",
    "        else: fusionMode = False\n",
    "        \n",
    "        #Save results to disk\n",
    "        dataPrintout, dataPrintoutNames = [sampleNames, sampleLabels, samplePredictions], ['Names', 'Labels', 'Raw Predictions']\n",
    "        if fusionMode: \n",
    "            dataPrintout.append(samplePredictionsFusion)\n",
    "            dataPrintoutNames.append('Fusion Predictions') \n",
    "        dataPrintout = pd.DataFrame(np.asarray(dataPrintout)).transpose()\n",
    "        dataPrintout.columns=dataPrintoutNames\n",
    "        dataPrintout.to_csv(self.dir_results + 'predictions_WSI.csv', index=False)\n",
    "        \n",
    "        #Evaluate original WSI results\n",
    "        computeClassificationMetrics(sampleLabels, samplePredictions, self.dir_results+'results_WSI_Initial')\n",
    "        if fusionMode: computeClassificationMetrics(sampleLabels, samplePredictionsFusion, self.dir_results+'results_WSI_Fusion')\n",
    "        \n",
    "        #If the labels/predictions should be mapped visually onto the WSI\n",
    "        if self.visualizeLabelGrids or self.visualizePredictionGrids:\n",
    "            for sampleIndex, sampleName in tqdm(enumerate(sampleNames), total=len(sampleNames), desc='Visualizing Results', leave=True, ascii=asciiFlag):\n",
    "                \n",
    "                #Get indices for the sample blocks\n",
    "                blockIndices = np.where(blockSampleNames == sampleName)[0]\n",
    "                \n",
    "                #Load the sample WSI\n",
    "                imageWSI = cv2.cvtColor(cv2.imread(WSIFilenames[sampleIndex], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "                if (len(self.cropData) != 0): \n",
    "                    cropData, paddingData = self.cropData[sampleIndex], self.paddingData[sampleIndex]\n",
    "                    imageWSI = np.pad(imageWSI[cropData[0]:cropData[1], cropData[2]:cropData[3]], ((paddingData[0], paddingData[1]), (paddingData[2], paddingData[3]), (0, 0)))\n",
    "                \n",
    "                #Create grid overlay objects: valueBenign(other)->green, valueMalignant->red, valueBackground->N/A (background blocks not classified or included)\n",
    "                if self.visualizeLabelGrids: gridOverlay_GT = np.zeros(imageWSI.shape, dtype=np.uint8)\n",
    "                gridOverlay_Predictions = np.zeros(imageWSI.shape, dtype=np.uint8)\n",
    "                gridOverlay_PredictionsFusion = np.zeros(imageWSI.shape, dtype=np.uint8)\n",
    "                for blockIndex in blockIndices:\n",
    "                    startRow, startColumn = blockLocations[blockIndex] \n",
    "                    posStart, posEnd = (startColumn, startRow), (startColumn+blockSize, startRow+blockSize)\n",
    "                    if self.visualizeLabelGrids: gridOverlay_GT = rectangle(gridOverlay_GT, posStart, posEnd, (255, 0, 0) if blockLabels[blockIndex] == valueMalignant else (0, 255, 0))\n",
    "                    gridOverlay_Predictions = rectangle(gridOverlay_Predictions, posStart, posEnd, (255, 0, 0) if blockPredictions[blockIndex] == valueMalignant else (0, 255, 0))\n",
    "                    gridOverlay_PredictionsFusion = rectangle(gridOverlay_PredictionsFusion, posStart, posEnd, (255, 0, 0) if blockPredictionsFusion[blockIndex] == valueMalignant else (0, 255, 0))\n",
    "                    \n",
    "                #Store overlays to disk\n",
    "                if self.visualizeLabelGrids: exportImage(self.dir_visuals_labelGrids+'overlay_labelGrid_'+sampleName+overlayExtension, gridOverlay_GT)\n",
    "                exportImage(self.dir_results_labelGrids+'overlay_predictionsGrid_'+sampleName+overlayExtension, gridOverlay_Predictions)\n",
    "                exportImage(self.dir_results_labelGrids+'overlay_fusionGrid_'+sampleName+overlayExtension, gridOverlay_PredictionsFusion)\n",
    "                \n",
    "                #Overlay grids on top of WSI and store to disk\n",
    "                if self.visualizeLabelGrids: \n",
    "                    imageWSI_GT = cv2.addWeighted(imageWSI, 1.0, gridOverlay_GT, overlayWeight, 0.0)\n",
    "                    exportImage(self.dir_visuals_overlaidLabelGrids+'labelGrid_'+sampleName+overlayExtension, imageWSI_GT)\n",
    "                imageWSI_Predictions = cv2.addWeighted(imageWSI, 1.0, gridOverlay_Predictions, overlayWeight, 0.0)\n",
    "                exportImage(self.dir_results_overlaidLabelGrids+'predictionsGrid_'+sampleName+overlayExtension, imageWSI_Predictions)\n",
    "                imageWSI_PredictionsFusion = cv2.addWeighted(imageWSI, 1.0, gridOverlay_PredictionsFusion, overlayWeight, 0.0)\n",
    "                exportImage(self.dir_results_overlaidLabelGrids+'fusionGrid_'+sampleName+overlayExtension, imageWSI_PredictionsFusion)\n",
    "    \n",
    "    #Train a new XGB Classifier model\n",
    "    def train(self, inputs, labels):\n",
    "        self.model_XGBClassifier = XGBClassifier(device=self.device)\n",
    "        _  = self.model_XGBClassifier.fit(inputs.astype(np.float32), labels)\n",
    "    \n",
    "    #Train on all available data and export models\n",
    "    def exportClassifier(self):\n",
    "        \n",
    "        #Setup, train, save, and clear the XGBClassifier model\n",
    "        self.train(self.blockFeatures, self.blockLabels)\n",
    "        \n",
    "        #Save to disk in .json format for easy reloading\n",
    "        self.model_XGBClassifier.save_model(dir_classifier_models + 'model_XGBClassifier.json')\n",
    "        \n",
    "        #Register converter for XGBClassifier\n",
    "        update_registered_converter(XGBClassifier, \"XGBoostXGBClassifier\", calculate_linear_classifier_output_shapes, convert_xgboost, options={\"nocl\": [True, False], \"zipmap\": [True, False, \"columns\"]},)\n",
    "        \n",
    "        #Convert classifier to onnx format and save to disk\n",
    "        model_onnx_XGBClassifier = to_onnx(self.model_XGBClassifier, self.blockFeatures.astype(np.float32), target_opset={\"\": skl2onnx.__max_supported_opset__, \"ai.onnx.ml\": 3})\n",
    "        with open(dir_classifier_models + 'model_XGBClassifier.onnx', 'wb') as f:f.write(model_onnx_XGBClassifier.SerializeToString())\n",
    "        \n",
    "        del self.model_XGBClassifier, model_onnx_XGBClassifier\n",
    "        if len(gpus) > 0: \n",
    "            torch.cuda.empty_cache() \n",
    "            cp._default_memory_pool.free_all_blocks()\n",
    "            \n",
    "        #Still need to export ResNet50 model to onnx as well for integration (will be leaving DenseNet and GradCam++ (decision fusion) aside, as they do not appear to impact the results)\n",
    "    \n",
    "    #Load a pretrained model\n",
    "    def loadClassifier(self):\n",
    "        self.model_XGBClassifier = XGBClassifier()\n",
    "        self.model_XGBClassifier.load_model(dir_classifier_models + 'model_XGBClassifier.json')\n",
    "        self.model_XGBClassifier._Booster.set_param({'device': self.device})\n",
    "        \n",
    "        #Using ONNX; can be done, but GPU handling here would need work to perform correctly/well in Python\n",
    "        #self.model_XGBClassifier = onnxruntime.InferenceSession(dir_classifier_models + 'model_XGBClassifier.onnx', providers = [('CUDAExecutionProvider', {\"device_id\": gpus[-1]}), 'CPUExecutionProvider'])\n",
    "        #if len(gpus) > 0: self.model_XGBClassifier.predict = lambda inputs: session.run(None, {'X': cp.asarray(inputs)})[0]\n",
    "        #else: self.model_XGBClassifier.predict = lambda inputs: session.run(None, {'X': inputs})[0]\n",
    "        \n",
    "    #Classify a sample WSI\n",
    "    def classifyWSI(self, evaluatePredictions):\n",
    "        \n",
    "        #Place data on the GPU if able\n",
    "        dataInput = np.asarray(self.blockFeatures.astype(np.float32))\n",
    "        if len(gpus) > 0: dataInput = cp.asarray(dataInput)\n",
    "        \n",
    "        #Classify blocks\n",
    "        if fusionMode_WSI: blockPredictions, blockPredictionsFusion = self.predict(dataInput, fusionMode_WSI, self.blockWeights)\n",
    "        else: blockPredictions, blockPredictionsFusion = self.predict(dataInput, fusionMode_WSI), []\n",
    "        blockPredictions, blockPredictionsFusion = np.asarray(blockPredictions), np.asarray(blockPredictionsFusion)\n",
    "        \n",
    "        #Clear the XGBClassifier model and data on GPU\n",
    "        del self.model_XGBClassifier, dataInput\n",
    "        if len(gpus) > 0: \n",
    "            torch.cuda.empty_cache() \n",
    "            cp._default_memory_pool.free_all_blocks()\n",
    "        \n",
    "        #Classify each WSI, that had its component blocks classified, according to specified threshold of allowable malignant blocks\n",
    "        sampleLabels, samplePredictions, samplePredictionsFusion, sampleBlockIndices = [], [], [], []\n",
    "        for sampleIndex, sampleName in enumerate(self.sampleNames):\n",
    "            blockIndices = np.where(self.blockSampleNames == sampleName)[0]\n",
    "            sampleBlockIndices.append(blockIndices)\n",
    "            samplePredictions.append((np.mean(blockPredictions[blockIndices]) >= thresholdWSI)*1)\n",
    "            if fusionMode_WSI: samplePredictionsFusion.append((np.mean(blockPredictionsFusion[blockIndices]) >= thresholdWSI)*1)\n",
    "        \n",
    "        #Convert list of WSI labels/predictions to arrays\n",
    "        samplePredictions = np.asarray(samplePredictions)\n",
    "        samplePredictionsFusion = np.asarray(samplePredictionsFusion)\n",
    "        \n",
    "        #Evaluate per-sample results\n",
    "        if evaluatePredictions: self.evaluateResultsWSI(self.sampleNames, self.WSIFilenames, self.WSILabels, samplePredictions, samplePredictionsFusion, self.blockSampleNames, None, blockPredictions, blockPredictionsFusion, self.blockLocations)\n",
    "        \n",
    "        #Create prediction arrays for reconstruction model input data and save them to disk\n",
    "        if classifierRecon:\n",
    "            predictionMaps, predictionsFusionMaps = [], []\n",
    "            for sampleIndex, sampleName in tqdm(enumerate(self.sampleNames), total=len(self.sampleNames), desc='Recon Data', leave=True, ascii=asciiFlag):\n",
    "                blockIndices = sampleBlockIndices[sampleIndex]\n",
    "                predictionLocations = self.blockLocations[blockIndices]//blockSize\n",
    "                predictionMap = np.full((self.shapeData[sampleIndex]), valueBackground)\n",
    "                predictionMap[predictionLocations[:,0], predictionLocations[:,1]] = blockPredictions[blockIndices]\n",
    "                predictionMaps.append(predictionMap)\n",
    "                if visualizeInputData_recon: exportImage(dir_recon_visuals_inputData+'predictionMap_'+sampleName+'.tif', cmapClasses(predictionMap)[:,:,:3].astype(np.uint8)*255)\n",
    "                if fusionMode_WSI:\n",
    "                    predictionFusionMap = np.full((self.shapeData[sampleIndex]), valueBackground)\n",
    "                    predictionFusionMap[predictionLocations[:,0], predictionLocations[:,1]] = blockPredictionsFusion[blockIndices]\n",
    "                    predictionsFusionMaps.append(predictionFusionMap)\n",
    "                    if visualizeInputData_recon: exportImage(dir_recon_visuals_inputData+'fusionMap_'+sampleName+'.tif', cmapClasses(predictionFusionMap)[:,:,:3].astype(np.uint8)*255)\n",
    "            predictionMaps = np.asarray(predictionMaps, dtype='object')\n",
    "            np.save(dir_recon_inputData + 'predictionMaps', predictionMaps)\n",
    "            if fusionMode_WSI: \n",
    "                predictionsFusionMaps = np.asarray(predictionsFusionMaps, dtype='object')\n",
    "                np.save(dir_recon_inputData + 'fusionMaps', predictionsFusionMaps)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aece8960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713f5a8f944b4109a60feb471a8b2a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature Extraction:   0%|          | 0/1129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cb339cadf543898e41aa8892ebe73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Saliency Maps:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e60702c0484dbaaff15c6cbbede938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Saliency Maps:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd60c5d78fa4f28810e2f0d58116365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block Classification:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd1bbee8ad34f5cab3d61125acf9b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Visualizing Results:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168a072c264d439885e7aab2df5d39f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block Extraction:   0%|          | 0/215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab0545479fb438ea3057cc548df24de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature Extraction:   0%|          | 0/4281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f904a7108e442fa554100269f1f796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Saliency Maps:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26df79af56e4a89b5f4aaaba0e95a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Saliency Maps:   0%|          | 0/215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e422a8c35d54e7889265e51be0f1aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Visualizing Results:   0%|          | 0/215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5727fc3e040e475a9c7c7d75893e9688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RANDS Data:   0%|          | 0/215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ff13eafd444b389dd5e45a0a426093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Block Extraction:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94479663cf540238a53822a606a6a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature Extraction:   0%|          | 0/1506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ace877c9880425bab6778929ed09e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Saliency Maps:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3fa412f2064ceab89114e4fd9f0a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Saliency Maps:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62b1a1e113444e08bb9125df0deba30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RANDS Data:   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'modelClassifier_WSI_RANDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m modelClassifier_RANDS\u001b[38;5;241m.\u001b[39mclassifyWSI(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#Clean RAM of larger object(s)\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m modelClassifier_WSI_RANDS\n\u001b[1;32m    154\u001b[0m cleanup()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelClassifier_WSI_RANDS' is not defined"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "#DATA_PROCESSING\n",
    "#==================================================================\n",
    "\n",
    "#Compute the ratio of minimum foreground to block area required for a block to be considered as holding foreground data\n",
    "blockBackgroundRatio = minimumForegroundArea/((blockSize*cameraResolution)**2)\n",
    "if blockBackgroundRatio > 1.0: sys.exit('\\nError - Minimum foreground area specified for foreground data exceeds the given block size.')\n",
    "\n",
    "#Define general labels and values to use\n",
    "labelBenign, labelMalignant, labelExclude = '0', '1', '2'\n",
    "valueBenign, valueMalignant, valueBackground = int(labelBenign), int(labelMalignant), 2\n",
    "cmapClasses = colors.ListedColormap(['lime', 'red', 'black'])\n",
    "\n",
    "#Determine overlay and grid image export extension; exporting as .tif for losselss and .jpg for compressed\n",
    "if overlayLossless: overlayExtension = '.tif'\n",
    "else: overlayExtension = '.jpg'\n",
    "\n",
    "#Determine offset to avoid overlapping squares in grid visualization\n",
    "gridThicknessOffset = gridThickness//2\n",
    "\n",
    "#Load sample metadata for blocks and WSI data; keep seperate until working with reconstruction model input data to prevent accidental evaluation of classifier model with training data\n",
    "try:\n",
    "    sampleNames_blocks, WSILabels_blocks = loadMetadata_WSI(file_blocks_metadataWSI)\n",
    "    WSILabels_blocks = WSILabels_blocks.astype(int)\n",
    "    WSIFilenames_blocks = np.asarray([dir_blocks_inputWSI + sampleName + '.jpg' for sampleName in sampleNames_blocks])\n",
    "except:\n",
    "    print('\\nWarning - There does not appear to be any metadata available for block samples.')\n",
    "    sampleNames_blocks, WSILabels_blocks, WSIFilenames_blocks = np.asarray([]), np.asarray([]), np.asarray([])\n",
    "try:\n",
    "    sampleNames_WSI, WSILabels_WSI = loadMetadata_WSI(file_WSI_metadataWSI)\n",
    "    WSILabels_WSI = WSILabels_WSI.astype(int)\n",
    "    WSIFilenames_WSI = np.asarray([dir_WSI_inputs + sampleName + '.jpg' for sampleName in sampleNames_WSI])\n",
    "except:\n",
    "    print('\\nWarning - There does not appear to be any metadata available for WSI samples.')\n",
    "    sampleNames_WSI, WSILabels_WSI, WSIFilenames_WSI = np.asarray([]), np.asarray([]), np.asarray([])\n",
    "\n",
    "#Combine sample names and WSI filenames for classifierRecon and/or blockBackgroundValue determination for classifierWSI\n",
    "sampleNames_recon, WSIFilenames_recon = np.concatenate([sampleNames_blocks, sampleNames_WSI]), np.concatenate([WSIFilenames_blocks, WSIFilenames_WSI])\n",
    "\n",
    "#If configured for block classification and/or model export\n",
    "if classifierBlocks or classifierExport: \n",
    "    \n",
    "    #Load and process metadata for available blocks and their originating WSI\n",
    "    blockSampleNames_blocks, indices_blocks, locations_blocks, blockLabels_blocks = loadMetadata_blocks(file_blocks_metadataBlocks)\n",
    "    blockLabels_blocks = blockLabels_blocks.astype(int)\n",
    "    blockNames_blocks = np.asarray([blockSampleNames_blocks[index] + '_' + indices_blocks[index] for index in range(0, len(blockSampleNames_blocks))])\n",
    "    blockFilenames_blocks = np.asarray([dir_blocks_inputBlocks + blockSampleNames_blocks[index] + os.path.sep + 'PS'+blockSampleNames_blocks[index]+'_'+str(indices_blocks[index])+'_'+str(locations_blocks[index, 0])+'_'+str(locations_blocks[index, 1])+'.tif' for index in range(0, len(blockSampleNames_blocks))])\n",
    "    \n",
    "    #Prepare classifier\n",
    "    modelClassifier_blocks = Classifier('blocks', blockNames_blocks, blockFilenames_blocks, blockSampleNames_blocks, locations_blocks, sampleNames_blocks, WSIFilenames_blocks, WSILabels_blocks, blockLabels=blockLabels_blocks)\n",
    "    \n",
    "    #Compute block features\n",
    "    modelClassifier_blocks.computeFeatures(overwrite_blocks_features)\n",
    "    \n",
    "    #Compute saliency maps for the blocks\n",
    "    if fusionMode_blocks: modelClassifier_blocks.computeSalicencyMaps(overwrite_blocks_saliencyMaps, visualizeSaliencyMaps_blocks)\n",
    "    \n",
    "    #Perform cross-validation\n",
    "    if classifierBlocks: modelClassifier_blocks.crossValidation()\n",
    "    \n",
    "    #Export classifier components\n",
    "    if classifierExport: modelClassifier_blocks.exportClassifier()\n",
    "        \n",
    "    #Clean RAM of larger object(s)\n",
    "    del modelClassifier_blocks\n",
    "    cleanup()\n",
    "\n",
    "#If classification of whole WSI or generation of reconstruction model input data should be performed\n",
    "if classifierWSI or classifierRecon: \n",
    "    \n",
    "    #If a background value for thresholding block data has not been set, determine one across all available WSI\n",
    "    if blockBackgroundValue == -1:\n",
    "        otsuThresholds = np.asarray([cv2.threshold(cv2.cvtColor(cv2.imread(filename, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2GRAY),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[0] for filename in tqdm(WSIFilenames_recon, desc='Background Value Determination', leave=True, ascii=asciiFlag)])\n",
    "        blockBackgroundValue = int(otsuThresholds.min())\n",
    "        print('For available WSI, the recommended value for blockBackgroundValue is: '+str(blockBackgroundValue))\n",
    "        \n",
    "    #If the overwrite is enabled, split WSI (excluding any that were used in training the classifier) into blocks and save to disk\n",
    "    if overwrite_WSI_blocks: \n",
    "        blockNames_WSI_WSI, blockFilenames_WSI_WSI, blockSampleNames_WSI_WSI, blockLocations_WSI_WSI, cropData_WSI_WSI, paddingData_WSI_WSI, shapeData_WSI_WSI = extractBlocks(WSIFilenames_WSI)\n",
    "        blockData_WSI = np.concatenate([np.expand_dims(blockNames_WSI_WSI, -1), np.expand_dims(blockFilenames_WSI_WSI, -1), np.expand_dims(blockSampleNames_WSI_WSI, -1), blockLocations_WSI_WSI], 1)\n",
    "        np.save(dir_WSI_blocks + 'blockData_WSI_WSI', blockData_WSI)\n",
    "        WSIData_WSI = np.concatenate([cropData_WSI_WSI, paddingData_WSI_WSI, shapeData_WSI_WSI], 1)\n",
    "        np.save(dir_WSI_blocks + 'WSIData_WSI_WSI', WSIData_WSI)\n",
    "    else: \n",
    "        blockData_WSI = np.load(dir_WSI_blocks + 'blockData_WSI_WSI.npy', allow_pickle=True)\n",
    "        blockNames_WSI_WSI, blockFilenames_WSI_WSI, blockSampleNames_WSI_WSI, blockLocations_WSI_WSI = np.split(blockData_WSI, [1, 2, 3], 1)\n",
    "        blockLocations_WSI_WSI = blockLocations_WSI_WSI.astype(int)\n",
    "        WSIData_WSI = np.load(dir_WSI_blocks + 'WSIData_WSI_WSI.npy', allow_pickle=True)\n",
    "        cropData_WSI_WSI, paddingData_WSI_WSI, shapeData_WSI_WSI = np.split(WSIData_WSI, [4, 8], 1)\n",
    "    \n",
    "    #Prepare classifier; loading the pre-trained model\n",
    "    modelClassifier_WSI = Classifier('WSI', blockNames_WSI_WSI, blockFilenames_WSI_WSI, blockSampleNames_WSI_WSI, blockLocations_WSI_WSI, sampleNames_WSI, WSIFilenames_WSI, WSILabels_WSI, cropData=cropData_WSI_WSI, paddingData=paddingData_WSI_WSI, shapeData=shapeData_WSI_WSI)\n",
    "    modelClassifier_WSI.loadClassifier()\n",
    "    \n",
    "    #Compute block features\n",
    "    modelClassifier_WSI.computeFeatures(overwrite_WSI_features, '_WSI_WSI')\n",
    "    \n",
    "    #Compute saliency maps for the blocks\n",
    "    if fusionMode_WSI: modelClassifier_WSI.computeSalicencyMaps(overwrite_WSI_saliencyMaps, visualizeSaliencyMaps_WSI, '_WSI_WSI')\n",
    "    \n",
    "    #Classify the WSI and evaluate results\n",
    "    if classifierWSI: modelClassifier_WSI.classifyWSI(True)\n",
    "    \n",
    "    #Clean RAM of larger object(s)\n",
    "    del modelClassifier_WSI\n",
    "    cleanup()\n",
    "\n",
    "#If data should be generated for reconstruction model, extract blocks/data for WSI used to train the classifier, merge with those not used, and export for later use\n",
    "if classifierRecon:\n",
    "    \n",
    "    #Either split WSI (excluding any that were used in training the classifier) into blocks and save to disk, or load data for those previously generated \n",
    "    if overwrite_WSI_blocks: \n",
    "        blockNames_WSI_blocks, blockFilenames_WSI_blocks, blockSampleNames_WSI_blocks, blockLocations_WSI_blocks, cropData_WSI_blocks, paddingData_WSI_blocks, shapeData_WSI_blocks = extractBlocks(WSIFilenames_blocks)\n",
    "        blockData_blocks = np.concatenate([np.expand_dims(blockNames_WSI_blocks, -1), np.expand_dims(blockFilenames_WSI_blocks, -1), np.expand_dims(blockSampleNames_WSI_blocks, -1), blockLocations_WSI_blocks], 1)\n",
    "        np.save(dir_WSI_blocks + 'blockData_WSI_blocks', blockData_blocks)\n",
    "        WSIData_blocks = np.concatenate([cropData_WSI_blocks, paddingData_WSI_blocks, shapeData_WSI_blocks], 1)\n",
    "        np.save(dir_WSI_blocks + 'WSIData_WSI_blocks', WSIData_blocks)\n",
    "        \n",
    "    else: \n",
    "        blockData_blocks = np.load(dir_WSI_blocks + 'blockData_WSI_blocks.npy', allow_pickle=True)\n",
    "        blockNames_WSI_blocks, blockFilenames_WSI_blocks, blockSampleNames_WSI_blocks, blockLocations_WSI_blocks = np.split(blockData_blocks, [1, 2, 3], 1)\n",
    "        blockLocations_WSI_blocks = blockLocations_WSI_blocks.astype(int)\n",
    "        WSIData_blocks = np.load(dir_WSI_blocks + 'WSIData_WSI_blocks.npy', allow_pickle=True)\n",
    "        cropData_WSI_blocks, paddingData_WSI_blocks, shapeData_WSI_blocks = np.split(WSIData_blocks, [4, 8], 1)\n",
    "    \n",
    "    #Prepare classifier\n",
    "    modelClassifier_WSI_blocks = Classifier('WSI', blockNames_WSI_blocks, blockFilenames_WSI_blocks, blockSampleNames_WSI_blocks, blockLocations_WSI_blocks, sampleNames_blocks, WSIFilenames_blocks, WSILabels_blocks, cropData=cropData_WSI_blocks, paddingData=paddingData_WSI_blocks, shapeData=shapeData_WSI_blocks)\n",
    "    \n",
    "    #Compute block features\n",
    "    modelClassifier_WSI_blocks.computeFeatures(overwrite_WSI_features, '_WSI_blocks')\n",
    "    \n",
    "    #Compute saliency maps for the blocks\n",
    "    if fusionMode_WSI: modelClassifier_WSI_blocks.computeSalicencyMaps(overwrite_WSI_saliencyMaps, visualizeSaliencyMaps_WSI, '_WSI_blocks')\n",
    "    \n",
    "    #Clean RAM of larger object(s)\n",
    "    del modelClassifier_WSI_blocks\n",
    "    cleanup()\n",
    "    \n",
    "    #Merge metadata for all WSI and store consolidated data to disk\n",
    "    WSILabels_recon = np.concatenate([WSILabels_blocks, WSILabels_WSI])\n",
    "    cropData_recon = np.concatenate([cropData_WSI_blocks, cropData_WSI_WSI])\n",
    "    paddingData_recon = np.concatenate([paddingData_WSI_blocks, paddingData_WSI_WSI])\n",
    "    shapeData_recon = np.concatenate([shapeData_WSI_blocks, shapeData_WSI_WSI])\n",
    "    WSIData_recon = np.concatenate([np.expand_dims(sampleNames_recon, -1), np.expand_dims(WSIFilenames_recon, -1), np.expand_dims(WSILabels_recon, -1), cropData_recon, paddingData_recon, shapeData_recon], 1)\n",
    "    np.save(dir_recon_inputData + 'WSIData_recon', WSIData_blocks)\n",
    "\n",
    "    #Merge metadata for all blocks and store consolidated data to disk\n",
    "    blockNames_recon = np.concatenate([blockNames_WSI_blocks, blockNames_WSI_WSI])\n",
    "    blockFilenames_recon = np.concatenate([blockFilenames_WSI_blocks, blockFilenames_WSI_WSI])\n",
    "    blockSampleNames_recon = np.concatenate([blockSampleNames_WSI_blocks, blockSampleNames_WSI_WSI])\n",
    "    blockLocations_recon = np.concatenate([blockLocations_WSI_blocks, blockLocations_WSI_WSI], dtype=np.int64) \n",
    "    blockData_recon = np.concatenate([np.atleast_2d(blockNames_recon.T).T, np.atleast_2d(blockFilenames_recon.T).T,  np.atleast_2d(blockSampleNames_recon.T).T, blockLocations_recon], 1)\n",
    "    np.save(dir_recon_inputData + 'blockData_recon', blockData_blocks)\n",
    "    \n",
    "    #Prepare classifier, loading the pre-trained model\n",
    "    modelClassifier_recon = Classifier('recon', blockNames_recon, blockFilenames_recon, blockSampleNames_recon, blockLocations_recon, sampleNames_recon, WSIFilenames_recon, WSILabels_recon, cropData_recon, paddingData_recon, shapeData_recon)\n",
    "    modelClassifier_recon.loadClassifier()\n",
    "    \n",
    "    #Classify the WSI; do not evaluate as they include WSI data used in training the classifier in the first place\n",
    "    modelClassifier_recon.classifyWSI(False)\n",
    "    \n",
    "    #Clean RAM of larger object(s)\n",
    "    del modelClassifier_recon\n",
    "    cleanup()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42d227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
