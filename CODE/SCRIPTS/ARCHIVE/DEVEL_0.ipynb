{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#Program: DEVEL_0\n",
    "#Version: 1.0\n",
    "#Author: David Helminiak\n",
    "#Date Created: August 18, 2024\n",
    "#Date Last Modified: August 30, 2024\n",
    "#Description: Re-implementation of a unified block classifier code for breast cancer data\n",
    "#Operation: Move back into main program directory before running.\n",
    "#Status: Deprecated - Needed to add abstraction; moved to DEVEL_1\n",
    "#==================================================================\n",
    "\n",
    "#Have the notebook fill more of the display width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:80% !important; }</style>\"))\n",
    "\n",
    "#Items otherwise covered when not running this code in a notebook\n",
    "import tempfile\n",
    "dir_tmp = tempfile.TemporaryDirectory(prefix='TMP_').name\n",
    "configFileName = './CONFIG_0-TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#NOTES\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "#WSI images are expected to be .jpg files and block images are expected to be .tif files\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#CONFIG PARAMETERS\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "#Should classification of blocks and their originating WSI be performed\n",
    "classifierTest = True\n",
    "\n",
    "#Should classifier model components be saved/exported\n",
    "classifierExport = True\n",
    "\n",
    "#Should WSI be segmented into blocks and classified; generates data for RANDS\n",
    "classifierWSI = True\n",
    "\n",
    "#Should WSI preparation and block extraction overwrite previously generated files (default: False)\n",
    "overwriteBlocksWSI = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CLASSIFICATION - BLOCK\n",
    "\n",
    "#How many samples should be submitted in a batch through pytorch models used in classifier; only used for inferencing (default: 1)\n",
    "#Incrementing in powers of 2 recommended to best leverage common GPU hardware designs\n",
    "#For ResNet and DenseNet a 2080TI 11GB reliably handles 64x3x224x224 (resizeImageSize=[224,224]) or 16x3x400x400 (resizeImageSize=[])\n",
    "batchsizeClassifier = 64\n",
    "\n",
    "#Specify what dimensions images should be resized to; if no resizing is desired leave as [] (default: [224, 224])\n",
    "#Leaving as [] will increase training time (also must change batchsizeClassifier), but can lead to improved scores\n",
    "#Original implementation uses [224, 224], though with adaptive average pooling this isn't actually neccessary\n",
    "resizeImageSize = [224, 224]\n",
    "\n",
    "#If folds for XGB classifier cross validation should be manually defined (e.g. [['S1', 'S3'], ['S4', 'S2']]), else use specify number of folds to generate\n",
    "#Default matches folds used in prior work (https://doi.org/10.3389/fonc.2023.1179025)\n",
    "#Omits 6 available samples, with folds holding: 11, 12, 12, 12, 13 samples respectively; this may have been to better balance class distribution\n",
    "#Presently, all available (non-excluded) samples (not just those in manualFolds) are currently used for training the exported/utilized final classifier\n",
    "manualFolds = [['2', '9', '11', '16', '34', '36', '40', '54', '57', '60', '62'],\n",
    "               ['17', '20', '23', '24', '28', '30', '33', '51', '52', '59', '63', '66'], \n",
    "               ['12', '14', '22', '26', '35', '44', '45', '47', '49', '53', '56', '68'], \n",
    "               ['4', '5', '8', '10', '25', '27', '29', '37', '42', '48', '50', '69'], \n",
    "               ['7', '15', '19', '31', '43', '46', '55', '58', '61', '64', '65', '67', '70']]\n",
    "\n",
    "#Which pre-trained weight sets should be used for ResNet and DenseNet model: 'IMAGENET1K_V1', 'IMAGENET1K_V2'\n",
    "#Unclear which weights were used for TensorFlow ResNet50 variant in original implementation, but V2 improves the score a bit when using resizeImageSize = []\n",
    "weightsResNet = 'IMAGENET1K_V2'\n",
    "\n",
    "#Which pre-trained weight sets should be used for DenseNet model: 'IMAGENET1K_V1'\n",
    "weightsDenseNet = 'IMAGENET1K_V1'\n",
    "\n",
    "#What ratio of malignant to benign blocks should be used to label a whole WSI as malignant\n",
    "#Unknown what the original work used for this value, but chose a value (in range of 0.12-0.19) that can replicate prior results\n",
    "thresholdWSI = 0.15\n",
    "\n",
    "\n",
    "#CLASSIFICATION - WSI\n",
    "\n",
    "#When splitting WSI images, what size should the resulting blocks be (default: [400, 400])\n",
    "blockSize = [400, 400]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RARELY CHANGED\n",
    "\n",
    "#Define labels used for normal/benign tissue\n",
    "#'a': normal adipose.\n",
    "#'s': normal stroma tissue excluding adipose.\n",
    "#'o': other normal tissue including parenchyma, adenosis, lobules, blood vessels, etc.\n",
    "benignLabels = ['a', 's', 'o', \n",
    "                'normal', \n",
    "                'fibroadenoma', \n",
    "                'normal breast tissue'\n",
    "               ]\n",
    "\n",
    "#Define labels used for malignant tissue\n",
    "#'d': IDC tumor\n",
    "#'l': ILC tumor\n",
    "#'ot': other tumor areas including DCIS, biopsy site, and slightly defocused tumor regions.\n",
    "malignantLabels = ['d', 'l', 'ot', \n",
    "                   'breast cancer', \n",
    "                   'IMC: Invasive Mucinous', \n",
    "                   'IDC', \n",
    "                   'IMC: IDC with lobular features', \n",
    "                   'IMC: ILC', \n",
    "                   'ILC', \n",
    "                   'Invasive lobular carcinoma'\n",
    "                  ]\n",
    "\n",
    "#Define labels used for tissues to be excluded\n",
    "#'ft': defocused but still visually tumor-like areas.\n",
    "#'f': severly out-of-focusing areas. \n",
    "#'b': background. \n",
    "#'e': bubbles.\n",
    "#Other labels used that were excluded from original work, so doing so here as well\n",
    "excludeLabels = ['ft', 'f', 'b', 'e',\n",
    "                'lymph node',\n",
    "                 'tongue tumor: invasive squamous cell carcinoma', \n",
    "                 'High grade carcinoma in lymph node', \n",
    "                 'osteosarcoma from mandible',\n",
    "                 'normal tongue'\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Already in setup CONFIG (avoid duplication during copy)\n",
    "#==================================================================\n",
    "#Should parallelization calls be used to leverage multithreading where able\n",
    "parallelization = True\n",
    "\n",
    "#If parallelization is enabled, how many CPU threads should be used in Ray tasks? (0 will use any/all available)\n",
    "#Recommend starting at half of the available system threads if using hyperthreading,\n",
    "#or 1-2 less than the number of system CPU cores if not using hyperthreading.\n",
    "#Adjust to where the CPU just below 100% usage during parallel operations \n",
    "availableThreads = 16\n",
    "\n",
    "#Which GPU(s) devices should be used (last specified used for data processing and model training); (default: [-1], any/all available; CPU only: [])\n",
    "gpus = [-1]\n",
    "\n",
    "#Should training/validation data be entirely stored on GPU (default: True; improves training/validation efficiency, set to False if OOM occurs)\n",
    "storeOnDevice = True\n",
    "   \n",
    "#RNG seed value to ensure run-to-run consistency (-1 to disable)\n",
    "manualSeedValue = 0\n",
    "\n",
    "debugMode = False\n",
    "\n",
    "asciiFlag = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#MAIN - Run codes that have already been completed\n",
    "#==================================================================\n",
    "\n",
    "exec(open(\"./CODE/EXTERNAL.py\", encoding='utf-8').read())\n",
    "\n",
    "exec(open(\"./CODE/COMPUTE.py\", encoding='utf-8').read())\n",
    "\n",
    "#Turn off image size checking; note that this can allow for decompression bomb DOS attacks if an untrusted image ends up as an input\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#MODEL_CLASS\n",
    "#==================================================================\n",
    "\n",
    "#Load and preprocess data image files\n",
    "class DataPreprocessing_Classifier(Dataset):\n",
    "    def __init__(self, filenames):\n",
    "        super().__init__()\n",
    "        self.filenames = filenames\n",
    "        self.numFiles = len(self.filenames)\n",
    "        if len(resizeImageSize) > 0:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                v2.Resize(tuple(resizeImageSize)), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else: \n",
    "             self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "    def __getitem__(self, index): return self.transform(Image.open(self.filenames[index]))\n",
    "\n",
    "    def __len__(self): return self.numFiles\n",
    "\n",
    "#Load/synchronize data labeling, drop excluded rows, and extract relevant metadata\n",
    "def loadMetadata(filename):\n",
    "    metadata = pd.read_excel(filename, header=None, names=['name', 'label'], converters={'name':str,'label':str})\n",
    "    metadata['label'].replace(benignLabels, benignLabel, inplace=True)\n",
    "    metadata['label'].replace(malignantLabels, malignantLabel, inplace=True)\n",
    "    metadata['label'].replace(excludeLabels, excludeLabel, inplace=True)\n",
    "    metadata = metadata.loc[metadata['label'] != excludeLabel]\n",
    "    return np.array(metadata['name']), np.array(metadata['label'])\n",
    "\n",
    "#Compute metrics for a classification result and visualize/save them as needed\n",
    "def computeClassificationMetrics(labels, predictions, displayLabels, filename):\n",
    "    classificationReport = classification_report(labels, predictions, target_names=displayLabels)\n",
    "    confusionMatrix = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = confusionMatrix.ravel()\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    print(classificationReport)\n",
    "    print('Accuracy: ', accuracy, '\\tSensitivity: ', sensitivity, '\\tSpecificity: ', specificity)\n",
    "    displayCM = ConfusionMatrixDisplay(confusionMatrix, display_labels=displayLabels)\n",
    "    displayCM.plot(cmap='Blues'); plt.show(); plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#INTERNAL_DIRECTORY\n",
    "#==================================================================\n",
    "\n",
    "#Indicate and setup the destination folder for results of this configuration\n",
    "destResultsFolder = './RESULTS_'+os.path.splitext(os.path.basename(configFileName).split('_')[1])[0]\n",
    "\n",
    "#If the folder already exists, either remove it, or append a novel value to it\n",
    "if os.path.exists(destResultsFolder):\n",
    "    if not preventResultsOverwrite: \n",
    "        shutil.rmtree(destResultsFolder)\n",
    "    else: \n",
    "        destinationNameValue = 0\n",
    "        destResultsFolder_Base = copy.deepcopy(destResultsFolder)\n",
    "        while True:\n",
    "            destResultsFolder = destResultsFolder_Base + '_' + str(destinationNameValue)\n",
    "            if not os.path.exists(destResultsFolder): break\n",
    "            destinationNameValue += 1\n",
    "\n",
    "\n",
    "#Set a base model name for the specified configuration\n",
    "modelName = 'model_RANDS_'\n",
    "\n",
    "#Data input directories/files\n",
    "dir_data = '.' + os.path.sep + 'DATA' + os.path.sep\n",
    "\n",
    "#Training and testing of block classification\n",
    "dir_dataBlocks = dir_data + 'BLOCKS' + os.path.sep\n",
    "dir_inputsBlocks = dir_dataBlocks + 'INPUTS' + os.path.sep\n",
    "file_labelsBlocks = dir_inputsBlocks + 'Patch_list.xlsx'\n",
    "\n",
    "#Testing of WSI classification and generation of RANDS data\n",
    "dir_dataWSI = dir_data + 'WSI' + os.path.sep\n",
    "dir_inputsWSI = dir_dataWSI + 'INPUTS' + os.path.sep\n",
    "dir_preparedWSI = dir_dataWSI + 'RESULTS_PREPARED_WSI' + os.path.sep\n",
    "dir_blocksWSI = dir_dataWSI + 'RESULTS_BLOCKS' + os.path.sep\n",
    "file_labelsWSI = dir_inputsWSI + 'WSI_list.xlsx'\n",
    "\n",
    "\n",
    "#Remove files/folders that are to be overwritten\n",
    "if overwriteBlocksWSI and os.path.exists(dir_blocksWSI): \n",
    "    shutil.rmtree(dir_blocksWSI)\n",
    "    os.makedirs(dir_blocksWSI)\n",
    "    shutil.rmtree(dir_preparedWSI)\n",
    "    os.makedirs(dir_preparedWSI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64bb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61a0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#DATA_PROCESSING\n",
    "#==================================================================\n",
    "\n",
    "#Define generalize labels used in place of the original, specific tissue labels\n",
    "benignLabel, malignantLabel, excludeLabel = 0, 1, 2\n",
    "\n",
    "#If features used in training the XGB Classifier model are needed (either for export or testing)\n",
    "if classifierExport or classifierTest: \n",
    "\n",
    "    #Load and process metadata for available blocks\n",
    "    blockNamesAll, blockLabelsAll = loadMetadata(file_labelsBlocks)\n",
    "    blockSampleNamesAll = np.array([re.split('PS|_', blockName)[1] for blockName in blockNamesAll])\n",
    "    blockFilenamesAll = [dir_inputsBlocks + 'S' + blockSampleNamesAll[blockIndex] + os.path.sep + blockNamesAll[blockIndex] + '.tif' for blockIndex in range(0, len(blockNamesAll))]\n",
    "\n",
    "    #Prepare data for use with PyTorch ResNet50 and DenseNet169 models\n",
    "    device = f\"cuda:{gpus[-1]}\" if len(gpus) > 0 else \"cpu\"\n",
    "    torchDevice = torch.device(device)\n",
    "    blockData = DataPreprocessing_Classifier(blockFilenamesAll)\n",
    "    blockDataloader = DataLoader(blockData, batch_size=batchsizeClassifier, num_workers=0, shuffle=False)#, pin_memory=True)\n",
    "    numBlockData = len(blockDataloader)\n",
    "\n",
    "    #==================================================================\n",
    "    #FEATURE EXTRACTION\n",
    "    #==================================================================\n",
    "\n",
    "    #Load pretrained resnet50 model and set to evaluation mode\n",
    "    model_ResNet = models.resnet50(weights=weightsResNet).to(device)\n",
    "    _ = model_ResNet.train(False)\n",
    "\n",
    "    #If classifier is to be exported, do so here\n",
    "    #if classifierExport: \n",
    "    \n",
    "    #Extract features for each batch of sample block images\n",
    "    blockFeaturesAll = []\n",
    "    for data in tqdm(blockDataloader, total=numBlockData, desc='Feature Extraction', leave=True, ascii=asciiFlag):\n",
    "        blockFeaturesAll += model_ResNet(data.to(torchDevice)).detach().cpu().tolist()\n",
    "\n",
    "    #Convert list of features to an array\n",
    "    blockFeaturesAll = np.asarray(blockFeaturesAll)\n",
    "\n",
    "    #Clear the ResNet model from the GPU\n",
    "    del model_ResNet\n",
    "    if len(gpus) > 0: torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "#==================================================================\n",
    "#CLASSIFICATION - XGBClassifier\n",
    "#==================================================================\n",
    "\n",
    "#If the classifier model should be tested\n",
    "if classifierTest: \n",
    "\n",
    "    #Allocate samples to folds for cross validation\n",
    "    if type(manualFolds) != list: folds = [array.tolist() for array in np.array_split(np.random.permutation(np.unique(blockSampleNamesAll)), manualFolds)]\n",
    "    else: folds = manualFolds\n",
    "    numFolds = len(folds)\n",
    "\n",
    "    #Split block features into specified folds, keeping track of originating indices and matched labels\n",
    "    foldsFeatures, foldsLabels, foldsSampleNames, foldsIndices = [], [], [], []\n",
    "    for fold in folds:\n",
    "        blockIndices = np.concatenate([np.where(blockSampleNamesAll == sampleName)[0] for sampleName in fold])\n",
    "        foldsIndices.append(blockIndices)\n",
    "        foldsFeatures.append(list(blockFeaturesAll[blockIndices]))\n",
    "        foldsLabels.append(list(blockLabelsAll[blockIndices]))\n",
    "        foldsSampleNames.append(list(blockSampleNamesAll[blockIndices]))\n",
    "\n",
    "    #Collapse indices, labels, and sample names for later evaluation of the fold data\n",
    "    blockIndices = np.concatenate(foldsIndices)\n",
    "    blockLabels = np.concatenate(foldsLabels)\n",
    "    blockSampleNames = np.concatenate(foldsSampleNames)\n",
    "\n",
    "    #Check class distribution between the folds\n",
    "    #print('B\\t M \\t Total')\n",
    "    #for foldNum in range(0, numFolds):\n",
    "    #    print(np.sum(np.array(foldsLabels[foldNum]) == 0),'\\t', np.sum(np.array(foldsLabels[foldNum]) == 1), '\\t', len(foldsLabels[foldNum]))\n",
    "\n",
    "    #Test on each fold, while training with all remaining, saving predictions for each block\n",
    "    #Note that GPU and CPU implementations of XGBClassifier use different algorithms and will yield different results\n",
    "    blockPredictions = []\n",
    "    for foldNum in tqdm(range(0, numFolds), desc='Block Classification', leave=True, ascii=asciiFlag):\n",
    "        model_XGBClassifier = XGBClassifier(device=device)\n",
    "        _  = model_XGBClassifier.fit(np.concatenate(foldsFeatures[:foldNum]+foldsFeatures[foldNum+1:]), np.concatenate(foldsLabels[:foldNum]+foldsLabels[foldNum+1:]))\n",
    "        blockPredictions += model_XGBClassifier.predict(np.asarray(foldsFeatures[foldNum])).tolist()\n",
    "\n",
    "    #Convert list of predictions to an array\n",
    "    blockPredictions = np.asarray(blockPredictions)\n",
    "\n",
    "    #Evaluate results\n",
    "    print('Initial Block Classifier Results')\n",
    "    computeClassificationMetrics(blockLabels, blockPredictions, ['Benign', 'Malignant'], None)\n",
    "\n",
    "    #==================================================================\n",
    "    #CLASSIFICATION - GradCam++\n",
    "    #==================================================================\n",
    "    #This section focues on reducing false positives on WSI classification and/or for novelty...\n",
    "    #It doesn't improve the per-block classification results and wouldn't be recommended for use in RANDS implementation...\n",
    "\n",
    "    #Load pre-trained DenseNet\n",
    "    model_DenseNet = models.densenet169(weights=weightsDenseNet)\n",
    "\n",
    "    #Replace the in-built classifier; unclear how this structure and these hyperparameters were determined\n",
    "    model_DenseNet.classifier = nn.Sequential(\n",
    "        nn.Linear(model_DenseNet.classifier.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, 2),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    model_DenseNet = model_DenseNet.to(torchDevice)\n",
    "    _ = model_DenseNet.train(False)\n",
    "\n",
    "    #Create GradCAMPlusPlus model; see https://github.com/jacobgil/pytorch-grad-cam for additional models and options\n",
    "    model_GradCamPlusPlus = GradCAMPlusPlus(model=model_DenseNet, target_layers=[model_DenseNet.features[-1]])\n",
    "\n",
    "    #If visualization of block saliency maps is going to be performed, then will need to define un-normalized transform for overlay target\n",
    "    if len(resizeImageSize)>0: transform = transforms.Compose([transforms.ToTensor(), v2.Resize(tuple(resizeImageSize))])\n",
    "    else: transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    #Compute confidence weights for each block\n",
    "    blockWeights = []\n",
    "    for data in tqdm(blockDataloader, total=numBlockData, desc='Weight Computation', leave=True, ascii=asciiFlag):\n",
    "\n",
    "        #Generate saliency maps for each block\n",
    "        blockSaliencyMaps = model_GradCamPlusPlus(input_tensor=data, targets=None)\n",
    "\n",
    "        #For current GradCamPlusPlus implementation, must manually clear internal copy of the outputs from GPU cache to prevent OOM\n",
    "        del model_GradCamPlusPlus.outputs \n",
    "        if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "\n",
    "        #Visualization of block saliency maps\n",
    "        #for index, blockFilename in enumerate(blockFilenamesAll[batchNum*batchsizeClassifier:(batchNum+1)*batchsizeClassifier]):\n",
    "        #    blockImage = np.moveaxis(transform(Image.open(blockFilename)).numpy(), 0, -1)\n",
    "        #    blockSaliencyMap = blockSaliencyMaps[index]\n",
    "        #    blockOverlaid = show_cam_on_image(blockImage, blockSaliencyMap, use_rgb=True)\n",
    "        #    plt.imshow(blockOverlaid); plt.show(); plt.close()\n",
    "\n",
    "        #Compute regional importance for each block as the average saliency map values\n",
    "        blockImportances = np.mean(blockSaliencyMaps, axis=(1,2))\n",
    "\n",
    "        #Threshold regional importance values by 0.25 to get block weights\n",
    "        blockWeights += list(np.where(blockImportances<0.25, 0, blockImportances))\n",
    "\n",
    "    #Convert list of new predictions to an array\n",
    "    blockWeights = np.asarray(blockWeights)\n",
    "\n",
    "    Tracer()\n",
    "    \n",
    "    #Multiply the block predictions (using -1 for benign and +1 for malignant) by the matching block weights\n",
    "    blockPredictionsNew = np.where(blockPredictions==0, -1, 1)*blockWeights[blockIndices]\n",
    "\n",
    "    #Identify any positive values as malignant and the remaining as benign\n",
    "    blockPredictionsNew = np.where(blockPredictionsNew>0, 1, 0)\n",
    "\n",
    "    #Evaluate new per-block results\n",
    "    print('Updated Block Classifier Results')\n",
    "    computeClassificationMetrics(blockLabels, blockPredictionsNew, ['Benign', 'Malignant'], None)\n",
    "\n",
    "\n",
    "    #==================================================================\n",
    "    #CLASSIFICATION - WSI\n",
    "    #==================================================================\n",
    "\n",
    "    sampleLabels, samplePredictions, samplePredictionsNew = [], [], []\n",
    "    for sampleName in np.unique(blockSampleNames):\n",
    "\n",
    "        #Find indices of samplename in sampleNames\n",
    "        sampleIndices = np.where(blockSampleNames == sampleName)[0]\n",
    "\n",
    "        #Determine if the number of malignant block predictions exceeds the configured threshold\n",
    "        sampleLabels.append((np.mean(blockLabels[sampleIndices]) >= thresholdWSI)*1)\n",
    "        samplePredictions.append((np.mean(blockPredictions[sampleIndices]) >= thresholdWSI)*1)\n",
    "        samplePredictionsNew.append((np.mean(blockPredictionsNew[sampleIndices]) >= thresholdWSI)*1)\n",
    "\n",
    "    #Convert list of WSI labels/predictions to arrays\n",
    "    sampleLabels = np.asarray(sampleLabels)\n",
    "    samplePredictions = np.asarray(samplePredictions)\n",
    "    samplePredictionsNew = np.asarray(samplePredictionsNew)\n",
    "\n",
    "    #Evaluate original WSI results\n",
    "    print('Initial WSI Classifier Results')\n",
    "    computeClassificationMetrics(sampleLabels, samplePredictions, ['Benign', 'Malignant'], None)\n",
    "\n",
    "    #Evaluate updated WSI results\n",
    "    print('Updated WSI Classifier Results')\n",
    "    computeClassificationMetrics(sampleLabels, samplePredictionsNew, ['Benign', 'Malignant'], None)\n",
    "    \n",
    "#If classifier model components (other than ResNet) are to be exported\n",
    "if classifierExport:\n",
    "    \n",
    "    #Train the XGBClassifier on all available block data\n",
    "    model_XGBClassifier = XGBClassifier(device=device)\n",
    "    _  = model_XGBClassifier.fit(blockFeaturesAll, blockLabelsAll)\n",
    "    \n",
    "    #Export/Store the XGBClassifier model here\n",
    "\n",
    "    #Clear the XGBClassifier model from the GPU\n",
    "    del model_XGBClassifier\n",
    "    if len(gpus) > 0: torch.cuda.empty_cache() \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c56de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8030d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#If WSI should be segmented into blocks and classified\n",
    "if classifierWSI:\n",
    "\n",
    "    #Either load blockFilenamesAll or split WSI to blocks and obtain such\n",
    "    #Original work resized WSI before splitting into blocks, which potentially introduced inconsistent artifacting from altering the native aspect ratio\n",
    "    if not overwriteBlocksWSI:\n",
    "        blockFilenamesAll = [natsort.natsorted(glob.glob(dir_sampleBlocksWSI+os.path.sep+'*.tif')) for dir_sampleBlocksWSI in natsort.natsorted(glob.glob(dir_blocksWSI+'*'))]\n",
    "    else: \n",
    "        \n",
    "        #Load and process metadata for available samples; could probably parallelize this if given time to do so\n",
    "        sampleNamesAll, sampleLabelsAll = loadMetadata(file_labelsWSI)\n",
    "        sampleFilenamesAll = [dir_inputsWSI + sampleNamesAll[sampleIndex] + '.jpg' for sampleIndex in range(0, len(sampleNamesAll))]\n",
    "\n",
    "        blockFilenamesAll = []\n",
    "        for filename in tqdm(sampleFilenamesAll, desc='WSI Block Extraction', leave=True, ascii=asciiFlag):\n",
    "\n",
    "            #Extract base sample name\n",
    "            sampleName = os.path.basename(filename).split('.jpg')[0]\n",
    "\n",
    "            #Load WSI image\n",
    "            imageWSI = np.asarray(Image.open(filename))\n",
    "\n",
    "            #Equalize image brightness with historgram equalization in YUV space and extract foreground with Otsu; no improvement, leaving for reference\n",
    "            #y, cr, cb = cv2.split(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2YCrCb))\n",
    "            #imageWSI = cv2.cvtColor(cv2.merge((cv2.equalizeHist(y), cr, cb)), cv2.COLOR_YCR_CB2RGB)\n",
    "            #mask = cv2.threshold(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "            \n",
    "            #Crop image to the largest foreground area\n",
    "            mask = cv2.threshold(cv2.cvtColor(imageWSI, cv2.COLOR_RGB2GRAY),0,255,cv2.THRESH_BINARY)[1]\n",
    "            contour = max(cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0], key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(contour) \n",
    "            imageWSI = copy.deepcopy(imageWSI)[y:y+h, x:x+w]\n",
    "\n",
    "            #Pad the image as needed (as symmetrially as possible) for an even division by the specified block size; compute numBlocks per row/column\n",
    "            padHeight = (int(np.ceil(imageWSI.shape[0]/blockSize[0]))*blockSize[0])-imageWSI.shape[0]\n",
    "            padWidth = (int(np.ceil(imageWSI.shape[1]/blockSize[1]))*blockSize[1])-imageWSI.shape[1]\n",
    "            padTop, padLeft = padHeight//2, padWidth//2\n",
    "            padBottom, padRight = padTop+(padHeight%2), padLeft+(padWidth%2)\n",
    "            imageWSI = np.pad(imageWSI, ((padTop, padBottom), (padLeft, padRight), (0, 0)))\n",
    "            numBlocksRow, numBlocksCol = imageWSI.shape[0]//blockSize[0], imageWSI.shape[1]//blockSize[1]\n",
    "\n",
    "            #Save processed WSI for ease of later visualization; Pillow with .tif extension saves perfectly lossless  (no loss of quality)\n",
    "            Image.fromarray(imageWSI).save(dir_preparedWSI + 'S' + sampleName + '.tif')\n",
    "\n",
    "            #Split the WSI into blocks and flatten\n",
    "            imageWSI = imageWSI.reshape(numBlocksRow, blockSize[0], numBlocksCol, blockSize[1], imageWSI.shape[2]).swapaxes(1,2)\n",
    "            imageWSI = imageWSI.reshape(-1, imageWSI.shape[2], imageWSI.shape[3], imageWSI.shape[4])\n",
    "\n",
    "            #Setup directory to store blocks and generate filenames/locations for each\n",
    "            dir_sampleBlocksWSI = dir_blocksWSI + 'S' + sampleName + os.path.sep\n",
    "            if not os.path.exists(dir_sampleBlocksWSI): os.makedirs(dir_sampleBlocksWSI)\n",
    "            blockFilenames = [dir_sampleBlocksWSI+'PS'+sampleName+'_'+str(rowNum)+'_'+str(colNum)+'.tif' for rowNum in range(0, numBlocksRow) for colNum in range(0, numBlocksCol)]\n",
    "            blockFilenamesAll.append(blockFilenames)\n",
    "\n",
    "            #Store blocks to disk; Pillow with .tif extension saves perfectly lossless  (no loss of quality)\n",
    "            for index in range(0, len(blockFilenames)): Image.fromarray(imageWSI[index]).save(blockFilenames[index])\n",
    "\n",
    "        del imageWSI\n",
    "        cleanup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa42a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd682959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Setup data loader\n",
    "\n",
    "#Load ResNet and extract features\n",
    "\n",
    "#Load XGB Classifier and classify the blocks; if background then set as benign...\n",
    "\n",
    "#Apply saliency mapping?\n",
    "\n",
    "#Output versions with and without 'fusion' method...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e2799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d26151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recombine split blocks (numBlocksRow, numBlocksCol, blockSize[0], blockSize[1], 3)\n",
    "#mergeWSI = splitWSI.swapaxes(1,2)\n",
    "#mergeWSI = mergeWSI.reshape(mergeWSI.shape[0]*mergeWSI.shape[1], mergeWSI.shape[2]*mergeWSI.shape[3], mergeWSI.shape[4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
