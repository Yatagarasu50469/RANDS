{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ec2536",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now classifying sample: 72_1\n",
      "  Final Fusion Prediction: Benign\n"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "#Program: WSI-Classifier-Reimplementation-Tyrell\n",
    "#Version: 1.4\n",
    "#Author: David Helminiak\n",
    "#Date Created: 4 April 2025\n",
    "#Date Last Modified: 12 September 2025\n",
    "#Description: Classify WSI .jpg files placed inside ./WSI/ folder\n",
    "#==================================================================\n",
    "\n",
    "#Have the notebook fill more of the display width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:80% !important; }</style>\"))\n",
    "\n",
    "#Load tracer for debugging\n",
    "from IPython.core.debugger import set_trace as Tracer\n",
    "\n",
    "#Configuration settings that do not generally need to be changed\n",
    "#====================================================================\n",
    "\n",
    "#Which channel should be used to assess whether patches are in the foreground: 'red', 'green', 'gray' (default: 'red')\n",
    "#'gray' used for original work replication\n",
    "channel_extraction = 'red'\n",
    "\n",
    "#What threshold should be used for patch extraction; minimum allowable % of chosen-channel (channel_extraction) values >= backgroundLevel (default: 0.2)\n",
    "#0.8 used for original work replication\n",
    "thresholdPatch_extraction = 0.2\n",
    "\n",
    "#Minimum chosen channel value [0, 255] for a location to contribute to the backgroundThreshold criteria (default: 5)\n",
    "backgroundLevel = 5\n",
    "\n",
    "#What ratio of malignant to benign patches should be used to predict a whole WSI as malignant (default: 0.15)\n",
    "#When set to 0, will label a WSI as malignant if any one patch is predicted as malignant\n",
    "#Unknown what the original work used for this value, but for original work replication, a value of 0.15 seems appropriate \n",
    "#Replication of original results occurs with values between 0.12-0.19\n",
    "thresholdWSI_prediction = 0.15\n",
    "\n",
    "#How many samples should be submitted in a batch through pytorch models used in classifier\n",
    "#Incrementing in powers of 2 recommended to best leverage common GPU hardware designs\n",
    "#For ResNet and DenseNet a 2080TI 11GB can handle 64x3x224x224 (resizeSize=224) or 16x3x400x400 (resizeSize=0)\n",
    "#In some cases, the GPU memory may not be fully released (even when explicitly told to do so), so using a lower batch size may help prevent OOM\n",
    "batchsizeClassifier = 32\n",
    "\n",
    "#Patch size for WSI-padding/extraction/classification\n",
    "patchSize = 400\n",
    "\n",
    "#Specify what symmetrical dimension patches should be resized to; if no resizing is desired leave as 0 (default: 224)\n",
    "#Leaving as 0 will increase training time (also must change batchsizeClassifier), but can lead to improved scores\n",
    "#Original implementation uses 224, though with adaptive average pooling this isn't actually neccessary\n",
    "resizeSize_patches = 224\n",
    "\n",
    "#Specify what symmetrical dimension WSI should be resized to when generating saliency maps (default: 224)\n",
    "#If fusion method were to be further developed, this should be changed to maintain the original sample aspect ratio. \n",
    "resizeSize_WSI = 224\n",
    "\n",
    "#How thick should the grid lines be when generating overlay images (defualt: 50)\n",
    "gridThickness = 50\n",
    "\n",
    "#Which pre-trained weight sets should be used for ResNet50 model: 'IMAGENET1K_V1', 'IMAGENET1K_V2'\n",
    "#Unclear which weights were used for TensorFlow ResNet50 variant in original classifier implementation\n",
    "#V2 did improve scores a bit in RANDS when using resizeSize = 0\n",
    "weightsResNet = 'IMAGENET1K_V2'\n",
    "\n",
    "#Which pre-trained weight sets should be used for DenseNet model: 'IMAGENET1K_V1'\n",
    "weightsDenseNet = 'IMAGENET1K_V1'\n",
    "\n",
    "#What weight should be used when overlaying data\n",
    "overlayWeight = 0.5\n",
    "\n",
    "#Which GPU(s) devices should be used (last specified used if model is not multi-GPU capable)\n",
    "#default: [-1], any/all available; CPU only: [])\n",
    "gpus = [-1]\n",
    "\n",
    "#Should parallelization calls be used to leverage multithreading where able\n",
    "#Not currently used in this program\n",
    "parallelization = True\n",
    "\n",
    "#If parallelization is enabled, how many CPU threads should be used? (0 will use any/all available)\n",
    "#Recommend starting at half of the available system threads if using hyperthreading,\n",
    "#or 1-2 less than the number of system CPU cores if not using hyperthreading.\n",
    "#Adjust to where the CPU just below 100% usage during parallel operations \n",
    "availableThreads = 0\n",
    "\n",
    "#Seed vale for repeatability\n",
    "manualSeedValue = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Imports \n",
    "#================================================================\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import copy\n",
    "import cupy as cp\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib\n",
    "import natsort\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Enivronment \n",
    "#================================================================\n",
    "\n",
    "#Setup deterministic behavior for torch (this alone does not affect CUDA-specific operations)\n",
    "if manualSeedValue != -1: torch.use_deterministic_algorithms(True, warn_only=False)\n",
    "\n",
    "#Detect logical and physical core counts, determining if hyperthreading is active\n",
    "logicalCountCPU = psutil.cpu_count(logical=True)\n",
    "physicalCountCPU = psutil.cpu_count(logical=False)\n",
    "hyperthreading = logicalCountCPU > physicalCountCPU\n",
    "\n",
    "#Set parallel CPU usage limit, disabling if there is only one thread remaining\n",
    "#Ray documentation indicates num_cpus should be out of the number of logical cores/threads\n",
    "#In practice, specifying a number closer to, or just below, the count of physical cores maximizes performance\n",
    "#Any ray.remote calls need to specify num_cpus to set environmental OMP_NUM_THREADS variable correctly\n",
    "if parallelization: \n",
    "    if availableThreads==0: numberCPUS = physicalCountCPU\n",
    "    else: numberCPUS = availableThreads\n",
    "    if numberCPUS <= 1: parallelization = False\n",
    "if not parallelization: numberCPUS = 1\n",
    "\n",
    "#Determine available GPUs \n",
    "if not torch.cuda.is_available(): gpus = []\n",
    "if (len(gpus) > 0) and (gpus[0] == -1): gpus = [*range(torch.cuda.device_count())]\n",
    "\n",
    "    \n",
    "#Internal variables \n",
    "#================================================================\n",
    "    \n",
    "#Define matplotlib colors for visualization\n",
    "cmapClasses = colors.ListedColormap(['lime', 'red', 'black'])\n",
    "\n",
    "#Determine offset to avoid overlapping squares in grid visualization\n",
    "gridThicknessOffset = gridThickness//2\n",
    "\n",
    "\n",
    "#Class and function definitions\n",
    "#================================================================\n",
    "\n",
    "#Define how to reset the random seed for deterministic repeatable RNG\n",
    "def resetRandom():\n",
    "    if manualSeedValue != -1:\n",
    "        torch.manual_seed(manualSeedValue)\n",
    "        torch.cuda.manual_seed_all(manualSeedValue)\n",
    "        np.random.seed(manualSeedValue)\n",
    "        random.seed(manualSeedValue)\n",
    "\n",
    "#OpenCV does not output sharp corners with its rectangle method...unless it's filled in\n",
    "def rectangle(image, startPos, endPos, color):\n",
    "    image = cv2.rectangle(image, startPos, endPos, color, -1)\n",
    "    image = cv2.rectangle(image, (startPos[0]+gridThicknessOffset, startPos[1]+gridThicknessOffset), (endPos[0]-gridThicknessOffset, endPos[1]-gridThicknessOffset), (0, 0, 0), -1)\n",
    "    return image\n",
    "\n",
    "#Convert numpy array to contiguous tensor; issues with lambda functions when using multiprocessing\n",
    "def contiguousTensor(inputs):\n",
    "    return torch.from_numpy(inputs).contiguous()\n",
    "\n",
    "#Rescale tensor; issues with lambda functions when using multiprocessing\n",
    "def rescaleTensor(inputs):\n",
    "    return inputs.to(dtype=torch.get_default_dtype()).div(255)\n",
    "\n",
    "#Generate a torch transform needed for preprocessing image data\n",
    "def generateTransform(resizeSize=[], rescale=False, normalize=False):\n",
    "    transform = [contiguousTensor]\n",
    "    if len(resizeSize) > 0: transform.append(v2.Resize(tuple(resizeSize)))\n",
    "    if rescale: transform.append(rescaleTensor)\n",
    "    if normalize: transform.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    return transforms.Compose(transform)\n",
    "\n",
    "#Load and preprocess data image files\n",
    "#ToTensor() swaps axes (so not used); convert to contiguous torch tensor manually\n",
    "#Rescaling and changing data type only after resizing (otherwise can escape [0, 1])\n",
    "class DataPreprocessing_Classifier(Dataset):\n",
    "    def __init__(self, filenames, resizeSize):\n",
    "        super().__init__()\n",
    "        self.filenames = filenames\n",
    "        self.numFiles = len(self.filenames)\n",
    "        if resizeSize > 0: self.transform = generateTransform([resizeSize, resizeSize], True, True)\n",
    "        else: self.transform = generateTransform([], True, True)\n",
    "        \n",
    "    #Rearranging import dimensions, allows resize transform before tensor-conversion/rescaling; preserves precision and output data range\n",
    "    def __getitem__(self, index): return self.transform(np.moveaxis(cv2.cvtColor(cv2.imread(self.filenames[index], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB), -1, 0))\n",
    "    \n",
    "    def __len__(self): return self.numFiles\n",
    "\n",
    "def get_total_space(pred_map):\n",
    "    pred_map = pred_map.astype(np.uint8)\n",
    "    pred_background = (pred_map.shape[0] * pred_map.shape[1]) - np.count_nonzero(pred_map)\n",
    "\n",
    "    connectivity = 8\n",
    "    output = cv2.connectedComponentsWithStats(pred_map, connectivity, cv2.CV_32S)\n",
    "    num_labels, _, stats, _ = output\n",
    "\n",
    "    return sum([stats[num, cv2.CC_STAT_AREA] for num in range(num_labels) if stats[num, cv2.CC_STAT_AREA] != pred_background])\n",
    "\n",
    "    \n",
    "#Create/load/run an available XGBClassifier model with ResNet50 feature extraction and GradCam++ using DenseNet169\n",
    "class Classifier_XGB_RN50_GCPP_DN169():\n",
    "    \n",
    "    def __init__(self, modelFile): \n",
    "        \n",
    "        #Set internal reference to model file\n",
    "        self.modelFile = modelFile\n",
    "        \n",
    "        #Setup available computation environment\n",
    "        self.device = f\"cuda:{gpus[-1]}\" if len(gpus) > 0 else \"cpu\"\n",
    "        self.torchDevice = torch.device(self.device)\n",
    "        \n",
    "        #Set default cuda device for XGBClassifier input data\n",
    "        if len(gpus) > 0: cp.cuda.Device(gpus[-1]).use()\n",
    "    \n",
    "    def classifyWSI(self, sampleName, imageWSI, patchLocations, patchNames, patchFilenames, dir_Results_Classification, dir_Results_Patches):\n",
    "        \n",
    "        #Start timer for data preparation\n",
    "        timePrepareStart = time.perf_counter()\n",
    "        \n",
    "        #Store needed input variables internally\n",
    "        self.sampleName = sampleName\n",
    "        self.imageWSI = imageWSI\n",
    "        self.patchLocations = patchLocations\n",
    "        self.dir_Results_Classification = dir_Results_Classification\n",
    "        \n",
    "        #Prepare data objects for obtaining/processing PyTorch model inputs\n",
    "        self.patchDataloader = DataLoader(DataPreprocessing_Classifier(patchFilenames, resizeSize_patches), batch_size=batchsizeClassifier, num_workers=0, shuffle=False, pin_memory=True)\n",
    "        if resizeSize_WSI > 0: transform = generateTransform([resizeSize_WSI, resizeSize_WSI], True, True)\n",
    "        else: transform = generateTransform([], True, True)\n",
    "        self.WSIDataloader = WSIDataloader = [item for item in DataLoader(transform(np.expand_dims(np.moveaxis(imageWSI, -1, 0), 0).copy()), batch_size=1, num_workers=0, shuffle=False, pin_memory=True)][0]\n",
    "        \n",
    "        #Log classifier preparation time\n",
    "        timePrepareDiff = time.perf_counter()-timePrepareStart\n",
    "        \n",
    "        #Start classification timer\n",
    "        timeClassifyStart = time.perf_counter()\n",
    "        \n",
    "        #Compute patch weights from DenseNet169/GradCam++ saliency map for fusion mechanism\n",
    "        self.computePatchWeights()\n",
    "\n",
    "        #Compute features for each patch using ResNet50\n",
    "        self.computeFeatures()\n",
    "        \n",
    "        #Classify patches\n",
    "        self.classifyPatches()\n",
    "        \n",
    "        #Classify WSI according ratio of malignant to foreground patches\n",
    "        if thresholdWSI_prediction == 0:\n",
    "            if np.sum(self.predictionsFusion) > 0: self.predictionFusionWSI = 1\n",
    "            else: self.predictionFusionWSI = 0\n",
    "        else: \n",
    "            if np.mean(self.predictionsFusion) >= thresholdWSI_prediction: self.predictionFusionWSI = 1\n",
    "            else: self.predictionFusionWSI = 0\n",
    "        \n",
    "        #Log classification time\n",
    "        timeClassifyDiff = time.perf_counter()-timeClassifyStart\n",
    "        \n",
    "        #Start timer for saving results to disk\n",
    "        timeSaveResultsStart = time.perf_counter()\n",
    "        \n",
    "        #Save patch predictions to disk\n",
    "        dataPrintout, dataPrintoutNames = [patchNames, self.predictions, self.predictionsFusion], ['Names', 'Predictions', 'Fusion Predictions']\n",
    "        dataPrintout = pd.DataFrame(np.asarray(dataPrintout)).transpose()\n",
    "        dataPrintout.columns=dataPrintoutNames\n",
    "        dataPrintout.to_csv(self.dir_Results_Classification + 'predictions_Patches.csv', index=False)\n",
    "        \n",
    "        #Save WSI predictions to disk\n",
    "        dataPrintout, dataPrintoutNames = [self.sampleName, self.predictionFusionWSI], ['Names', 'Fusion Prediction']\n",
    "        dataPrintout = pd.DataFrame(np.asarray(dataPrintout)).transpose()\n",
    "        dataPrintout.columns=dataPrintoutNames\n",
    "        dataPrintout.to_csv(self.dir_Results_Classification + 'predictions_WSI.csv', index=False)\n",
    "        \n",
    "        #Log results saving time \n",
    "        timeSaveResultsDiff = time.perf_counter()-timeSaveResultsStart\n",
    "        \n",
    "        #Start timer for visualization generation\n",
    "        timeVisualizationStart = time.perf_counter()\n",
    "        \n",
    "        #Create grid overlays\n",
    "        gridOverlay_Predictions = np.zeros(self.imageWSI.shape, dtype=np.uint8)\n",
    "        colorsPredictions = (cmapClasses(self.predictions)[:,:3].astype(np.uint8)*255).tolist()\n",
    "        gridOverlay_PredictionsFusion = np.zeros(self.imageWSI.shape, dtype=np.uint8)\n",
    "        colorsFusion = (cmapClasses(self.predictionsFusion)[:,:3].astype(np.uint8)*255).tolist()\n",
    "        for patchIndex in range(0, len(patchLocations)):\n",
    "            startRow, startColumn = patchLocations[patchIndex] \n",
    "            posStart, posEnd = (startColumn, startRow), (startColumn+patchSize, startRow+patchSize)\n",
    "            gridOverlay_Predictions = rectangle(gridOverlay_Predictions, posStart, posEnd, colorsPredictions[patchIndex])\n",
    "            gridOverlay_PredictionsFusion = rectangle(gridOverlay_PredictionsFusion, posStart, posEnd, colorsFusion[patchIndex])\n",
    "        \n",
    "        #Store raw overlays to disk\n",
    "        filename = self.dir_Results_Classification+'gridPredictions_'+self.sampleName+'.tif'\n",
    "        cv2.imwrite(filename, cv2.cvtColor(gridOverlay_Predictions, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        filename = self.dir_Results_Classification+'gridPredictionsFusion_'+self.sampleName+'.tif'\n",
    "        cv2.imwrite(filename, cv2.cvtColor(gridOverlay_PredictionsFusion, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        \n",
    "        #Overlay grids on top of WSI and store to disk\n",
    "        filename = self.dir_Results_Classification+'overlaidPredictions_'+self.sampleName+'.tif'\n",
    "        imageWSI_Prediction = cv2.addWeighted(self.imageWSI, 1.0, gridOverlay_Predictions, overlayWeight, 0.0)\n",
    "        cv2.imwrite(filename, cv2.cvtColor(imageWSI_Prediction, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        filename = self.dir_Results_Classification+'overlaidPredictionsFusion_'+self.sampleName+'.tif'\n",
    "        imageWSI_PredictionsFusion = cv2.addWeighted(self.imageWSI, 1.0, gridOverlay_PredictionsFusion, overlayWeight, 0.0)\n",
    "        cv2.imwrite(filename, cv2.cvtColor(imageWSI_PredictionsFusion, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        \n",
    "        #Log visualization time \n",
    "        timeVisualizationDiff = time.perf_counter()-timeVisualizationStart\n",
    "        \n",
    "        #Clear remaining internal objects that are not required after classification to save memory\n",
    "        del self.sampleName, self.imageWSI, self.patchLocations, self.dir_Results_Classification, self.predictions, self.predictionsFusion\n",
    "        \n",
    "        return self.predictionFusionWSI, imageWSI_Prediction, imageWSI_PredictionsFusion\n",
    "    \n",
    "    def computeFeatures(self):\n",
    "        \n",
    "        #Reset RNG before setting up model; else initialization values may be inconsistent\n",
    "        resetRandom()\n",
    "        \n",
    "        #Load pretrained ResNet50 model and set to evaluation mode\n",
    "        model_ResNet = models.resnet50(weights=weightsResNet).to(self.torchDevice)\n",
    "        _ = model_ResNet.train(False)\n",
    "\n",
    "        #Extract features for each batch of sample patch images\n",
    "        self.patchFeatures = []\n",
    "        for data in self.patchDataloader: self.patchFeatures += model_ResNet(data.to(self.torchDevice)).detach().cpu().tolist()\n",
    "        \n",
    "        #Clear the ResNet model and patch dataloader\n",
    "        del model_ResNet, self.patchDataloader\n",
    "        if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "        \n",
    "        #Convert list of features to an array\n",
    "        self.patchFeatures = np.asarray(self.patchFeatures)\n",
    "        \n",
    "        #Save features to disk\n",
    "        np.save(self.dir_Results_Classification + 'patchFeatures', self.patchFeatures)\n",
    "    \n",
    "    def computePatchWeights(self):\n",
    "        \n",
    "        #Reset RNG before setting up model; else initialization values may be inconsistent\n",
    "        resetRandom()\n",
    "            \n",
    "        #Load pre-trained DenseNet\n",
    "        model_DenseNet = models.densenet169(weights=weightsDenseNet)\n",
    "        \n",
    "        #Replace the in-built classifier; unclear how this structure and these hyperparameters were determined\n",
    "        model_DenseNet.classifier = nn.Sequential(\n",
    "            nn.Linear(model_DenseNet.classifier.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        model_DenseNet = model_DenseNet.to(self.torchDevice)\n",
    "        _ = model_DenseNet.train(False)\n",
    "        \n",
    "        #Create GradCAMPlusPlus model; see https://github.com/jacobgil/pytorch-grad-cam for additional models and options\n",
    "        model_GradCamPlusPlus = GradCAMPlusPlus(model=model_DenseNet, target_layers=[model_DenseNet.features[-1]])\n",
    "        \n",
    "        #Compute saliency map for the WSI\n",
    "        #For current GradCamPlusPlus implementation, must manually clear internal copy of the outputs from GPU cache to prevent OOM\n",
    "        #Do not need to move data to device here, as managed by GradCAMPlusPlus (having already been placed on device)\n",
    "        saliencyMap = model_GradCamPlusPlus(input_tensor=self.WSIDataloader, targets=None)\n",
    "        del model_GradCamPlusPlus.outputs\n",
    "        if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "        \n",
    "        #Clear DenseNet, GradCamPlusPlus, and the WSI data loader\n",
    "        del model_DenseNet, model_GradCamPlusPlus, self.WSIDataloader\n",
    "        if len(gpus) > 0: torch.cuda.empty_cache()\n",
    "        \n",
    "        #Store the raw saliency map to disk\n",
    "        self.saliencyMap = copy.deepcopy(saliencyMap)\n",
    "        image = matplotlib.cm.jet(saliencyMap)[0,:,:,:-1].astype(np.float32)\n",
    "        filename = self.dir_Results_Classification+'saliencyMap_'+self.sampleName+'.tif'\n",
    "        cv2.imwrite(filename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        \n",
    "        #Resize the saliency map to match the WSI dimensions\n",
    "        transform = generateTransform(self.imageWSI.shape[:2], False, False)\n",
    "        saliencyMap = transform(np.expand_dims(saliencyMap, 0))[0][0].numpy()\n",
    "        \n",
    "        #Overlay the saliency map on the WSI (grayscale for clearer visualization) and save it to disk\n",
    "        overlaid = np.expand_dims(cv2.cvtColor(self.imageWSI, cv2.COLOR_RGB2GRAY), -1)\n",
    "        transform = generateTransform([], True, False)\n",
    "        overlaid = np.moveaxis(transform(np.moveaxis(overlaid, -1, 0)).numpy(), 0, -1)\n",
    "        overlaid = show_cam_on_image(overlaid, saliencyMap, use_rgb=True, colormap=cv2.COLORMAP_HOT, image_weight=1.0-overlayWeight)\n",
    "        self.overlaid = overlaid\n",
    "        filename = self.dir_Results_Classification+'overlaidSaliency_'+self.sampleName+'.tif'\n",
    "        cv2.imwrite(filename, cv2.cvtColor(overlaid, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "        \n",
    "        #Extract saliency map data specific to sample patch locations, compute regional importance as the average value, and threshold to get weights\n",
    "        self.patchWeights = []\n",
    "        for locationData in self.patchLocations:\n",
    "            startRow, startColumn = locationData\n",
    "            patchImportance = np.mean(saliencyMap[startRow:startRow+patchSize, startColumn:startColumn+patchSize])\n",
    "            if patchImportance < 0.25: self.patchWeights.append(0)\n",
    "            else: self.patchWeights.append(patchImportance)\n",
    "        \n",
    "        #Convert list of patch weights to an array and save to disk\n",
    "        self.patchWeights = np.asarray(self.patchWeights)\n",
    "        np.save(self.dir_Results_Classification + 'patchWeights', self.patchWeights)\n",
    "    \n",
    "    #Classify patches and perform decision fusion\n",
    "    def classifyPatches(self):\n",
    "    \n",
    "        #Load a pretrained XGBClassifier model\n",
    "        model_XGBClassifier = XGBClassifier()\n",
    "        model_XGBClassifier.load_model(self.modelFile)\n",
    "        model_XGBClassifier._Booster.set_param({'device': self.device})\n",
    "        \n",
    "        #Place data on the GPU if able\n",
    "        dataInput = np.asarray(self.patchFeatures.astype(np.float32))\n",
    "        if len(gpus) > 0: dataInput = cp.asarray(dataInput)\n",
    "        \n",
    "        #Compute the raw patch predictions\n",
    "        self.predictions = model_XGBClassifier.predict(dataInput)\n",
    "        self.predictions = np.asarray(self.predictions)\n",
    "        \n",
    "        #Clear the XGBClassifier model and data on GPU\n",
    "        del model_XGBClassifier, dataInput\n",
    "        if len(gpus) > 0: \n",
    "            torch.cuda.empty_cache() \n",
    "            cp._default_memory_pool.free_all_blocks()\n",
    "        \n",
    "        #Perform decision fusion, multiplying predictions with saliency map weights (using -1 for benign and +1 for malignant)\n",
    "        self.predictionsFusion = np.where(self.predictions==0, -1, 1)*self.patchWeights\n",
    "        self.countSignificant = np.count_nonzero(self.predictionsFusion)\n",
    "        self.predictionsFusion = np.where(self.predictionsFusion>0, 1, 0)\n",
    "        del self.patchWeights\n",
    "\n",
    "#Extract patches from WSI and save thenm\n",
    "def extractPatches(imageWSI, dir_patches):\n",
    "\n",
    "    #Split the WSI into patches and flatten\n",
    "    numPatchesRow, numPatchesCol = imageWSI.shape[0]//patchSize, imageWSI.shape[1]//patchSize\n",
    "    imageWSI = imageWSI.reshape(numPatchesRow, patchSize, numPatchesCol, patchSize, imageWSI.shape[2]).swapaxes(1,2)\n",
    "    imageWSI = imageWSI.reshape(-1, imageWSI.shape[2], imageWSI.shape[3], imageWSI.shape[4])\n",
    "    \n",
    "    #Save patches to disk (always lossless) that meet a given threshold of chosen-channel values at or over a given background level\n",
    "    patchLocations, patchNames, patchFilenames = [], [], []\n",
    "    patchIndex = 0\n",
    "    for rowNum in range(0, numPatchesRow):\n",
    "        for colNum in range(0, numPatchesCol):\n",
    "            image = imageWSI[patchIndex]\n",
    "            if channel_extraction == 'red': channelValue = np.mean(image[:,:,0] >= backgroundLevel)\n",
    "            elif channel_extraction == 'green': channelValue = np.mean(image[:,:,1] >= backgroundLevel)\n",
    "            elif channel_extraction == 'gray': channelValue = np.mean(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) >= backgroundLevel)\n",
    "            else: sys.exit('Error - Unknown channel selected for use during patch extraction')\n",
    "            if channelValue > thresholdPatch_extraction:\n",
    "                locationRow, locationColumn= rowNum*patchSize, colNum*patchSize\n",
    "                patchLocations.append([locationRow, locationColumn])\n",
    "                patchName = sampleName+'_'+str(patchIndex)+'_'+str(locationRow)+'_'+str(locationColumn)\n",
    "                patchNames.append(patchName)\n",
    "                patchFilename = dir_patches + 'PS' + patchName + '.tif'\n",
    "                writeSuccess = cv2.imwrite(patchFilename, cv2.cvtColor(image, cv2.COLOR_RGB2BGR), params=(cv2.IMWRITE_TIFF_COMPRESSION, 1))\n",
    "                patchFilenames.append(patchFilename)\n",
    "            patchIndex += 1\n",
    "\n",
    "    return patchLocations, patchNames, patchFilenames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MAIN PROGRAM\n",
    "#===========================================================================\n",
    "#Specify internal paths\n",
    "dir_WSI = './Full_INPUT_WSI/'\n",
    "dir_Results = './RESULTS/'\n",
    "if not os.path.exists(dir_Results): os.mkdir(dir_Results)\n",
    "\n",
    "#Create a new classifier object\n",
    "classifier = Classifier_XGB_RN50_GCPP_DN169('./model_XGBClassifier.json')\n",
    "\n",
    "#Classify each WSI with a .jpg extension in the \n",
    "for filename in natsort.natsorted(glob.glob(dir_WSI + '*.jpg')):\n",
    "\n",
    "    sampleName = os.path.basename(filename).split('.')[0]\n",
    "    print('Now classifying sample: ' + sampleName)\n",
    "\n",
    "    #Create output folders for the current sample; will overwrite prior results\n",
    "    dir_resultsWSI = dir_Results + sampleName\n",
    "    dir_patches = dir_resultsWSI + '/Patches/'\n",
    "    dir_classification = dir_resultsWSI + '/Classification/'\n",
    "    if os.path.exists(dir_resultsWSI): shutil.rmtree(dir_resultsWSI)\n",
    "    os.mkdir(dir_resultsWSI)\n",
    "    os.mkdir(dir_patches)\n",
    "    os.mkdir(dir_classification)\n",
    "\n",
    "    #Load in the WSI (RGB ordering)\n",
    "    imageWSI = cv2.cvtColor(cv2.imread(filename, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #Crop off the right and bottom edges for even division by the specified patch size\n",
    "    numPatchesRow, numPatchesCol = imageWSI.shape[0]//patchSize, imageWSI.shape[1]//patchSize\n",
    "    imageWSI = imageWSI[:numPatchesRow*patchSize, :numPatchesCol*patchSize]\n",
    "    \n",
    "    #Extract patches\n",
    "    patchLocations, patchNames, patchFilenames = extractPatches(imageWSI, dir_patches)\n",
    "\n",
    "    #Classify patches and WSI\n",
    "    predictionFusionWSI, imageWSI_Prediction, imageWSI_PredictionsFusion = classifier.classifyWSI(sampleName, imageWSI, patchLocations, patchNames, patchFilenames, dir_classification, dir_patches)\n",
    "    if predictionFusionWSI == 1: print('  Final Fusion Prediction: Malignant')\n",
    "    else: print('  Final Fusion Prediction: Benign')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f36e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
